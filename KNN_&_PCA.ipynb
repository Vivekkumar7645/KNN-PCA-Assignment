{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Q1.What is K-Nearest Neighbors (KNN) and how does it work.\n",
        "**Ans**- K-Nearest Neighbors is a supervised machine learning algorithm that can be used for classification or regression. It's one of the simplest, yet surprisingly effective algorithms.\n",
        "\n",
        "**KNN Working**\n",
        "1. Choose the number of neighbors\n",
        "* We pick how many neighboring points to look at.\n",
        "2. Calculate the distance\n",
        "* Measure how far the new point is from all existing points using a distance metric:\n",
        "  * Euclidean distance\n",
        "  * Manhattan distance\n",
        "  * Minkowski distance\n",
        "\n",
        "Formula for Euclidean distance (in 2D):\n",
        "\n",
        "    d = √((x₂-x₁)²+(y₂-y₁)²)\n",
        "3. Find the K nearest neighbors\n",
        "* Sort the dataset by distance and pick the top 'K' closest points.\n",
        "\n",
        "4. Vote or average\n",
        "* For classification:\n",
        "\n",
        "Take a majority vote among the neighbors' labels.\n",
        "\n",
        "* For regression:\n",
        "\n",
        "Take the average of the neighbors' values.\n",
        "\n",
        "5. Assign the result\n",
        "The new point is given the label or value based on its neighbors.\n",
        "\n",
        "**Example (Classification)**\n",
        "\n",
        "Imagine we have a dataset of fruits labeled by weight and color — and labeled as apple or orange.\n",
        "\n",
        "We get a new fruit and want to classify it:\n",
        "* Measure distance to all other fruits.\n",
        "* Pick the 3 nearest.\n",
        "* If 2 out of 3 are apples → classify as an apple.\n",
        "\n",
        "**Advantages**\n",
        "* Simple to understand and implement.\n",
        "* No need for a trained model — it's lazy learning.\n",
        "* Works well with small, clean datasets.\n",
        "\n",
        "**Disadvantages**\n",
        "* Can be slow with large datasets.\n",
        "* Sensitive to irrelevant features and different scales.\n",
        "* Choice of K can affect accuracy."
      ],
      "metadata": {
        "id": "NfmX4DSVpqaG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q2.What is the difference between KNN Classification and KNN Regression.\n",
        "**Ans** - while both use the K-Nearest Neighbors idea, what they do with the neighbors is different.\n",
        "\n",
        "**KNN Classification vs KNN Regression**\n",
        "\n",
        "|Aspect\t|KNN Classification\t|KNN Regression|\n",
        "|-|||\n",
        "|Type of Output\t|Categorical (like Apple, Orange, Yes, No)\t|Continuous (like price, temperature, height)|\n",
        "|Decision Rule\t|Majority vote — whichever class is most common among the K neighbors\t|Average (or weighted average) of the K neighbors' values|\n",
        "|Use Case\t|Spam detection, disease prediction, image recognition\t|House price prediction, temperature forecasting|\n",
        "|Example\t|If 3 out of 5 neighbors are labeled Cat, classify as Cat\t|If neighbor values are [10, 12, 14], predict average → 12|\n",
        "|Evaluation Metrics\t|Accuracy, Precision, Recall, F1-score\t|Mean Squared Error (MSE), Mean Absolute Error (MAE), R² score|\n",
        "\n",
        "**Visual Intuition**\n",
        "* Classification:\n",
        "  * New point → Look at K nearest points → Pick the class that appears most.\n",
        "* Regression:\n",
        "  * New point → Look at K nearest points → Average their values.\n",
        "\n",
        "**Quick Example**\n",
        "\n",
        "Dataset\n",
        "\n",
        "|Weight\t|Color Value\t|Label (Classification)\t|Value (Regression)|\n",
        "|-||||\n",
        "|200g\t|0.7 (greenish)\t|Apple\t|₹50|\n",
        "|250g\t|0.9 (reddish)\t|Apple\t|₹60|\n",
        "|300g\t|0.4 (orange)\t|Orange\t|₹70|\n",
        "\n",
        "New fruit: 260g, 0.85 color\n",
        "* KNN Classification:\n",
        "  * Nearest neighbors: Apple, Apple, Orange\n",
        "  * Result: Apple (2 votes vs 1)\n",
        "* KNN Regression:\n",
        "  * Nearest values: ₹60, ₹50, ₹70\n",
        "  * Average: ₹60"
      ],
      "metadata": {
        "id": "rNppUybSpyfm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q3. What is the role of the distance metric in KNN?\n",
        "**Ans** - The distance metric is the mathematical formula KNN uses to measure how close or similar two data points are in feature space.\n",
        "Since KNN is based on the nearest neighbors, it defines near depends entirely on this metric.\n",
        "\n",
        "Suppose We are trying to find our closest friends in a city based on how far their houses are from me - what we consider “distance” matters.\n",
        "\n",
        "**Common Distance Metrics in KNN**\n",
        "1. Euclidean Distance\n",
        "  * Straight-line distance between two points.\n",
        "\n",
        "Formula (in 2D)\n",
        "\n",
        "    d = √((x₂−x₁)²+(y₂−y₁)²)\n",
        "\n",
        "Good for\n",
        "Continuous, real-valued features.\n",
        "\n",
        "2. Manhattan Distance\n",
        "  * Sum of absolute differences between coordinates\n",
        "\n",
        "Formula\n",
        "\n",
        "    d = |x₂-x₁|+|y₂-y₁|\n",
        "Good for\n",
        "Grid-like data or when movement is restricted to orthogonal directions.\n",
        "\n",
        "3. Minkowski Distance\n",
        "  * A generalization of both Euclidean and Manhattan.\n",
        "\n",
        "Formula\n",
        "\n",
        "    d = (∑ⁿᵢ₌₁ |xᵢ-yᵢ|ᵖ)¹/ᵖ\n",
        "\n",
        "* p=1 → Manhattan\n",
        "* p=2 → Euclidean\n",
        "* p=3 or higher → more flexible\n",
        "\n",
        "4. Hamming Distance\n",
        "  * Counts the number of positions where values differ."
      ],
      "metadata": {
        "id": "xfIgaDQ6pzRf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q4. What is the Curse of Dimensionality in KNN?\n",
        "**Ans** - The Curse of Dimensionality refers to the weird, unintuitive problems that happen when our data has too many features.\n",
        "\n",
        "KNN relies on measuring distances to find nearby points. But in high-dimensional spaces:\n",
        "* All points tend to become equally far apart — meaning, \"neighbors\" stop feeling like neighbors.\n",
        "* Distance-based algorithms like KNN struggle to find meaningful nearby points because everything seems far.\n",
        "\n",
        "**Reason**\n",
        "\n",
        "As we add more dimensions:\n",
        "* The volume of the space increases exponentially.\n",
        "* Data points become sparse.\n",
        "* The concept of \"closeness\" breaks down — most points are about the same distance from each other.\n",
        "\n",
        "Example:\n",
        "\n",
        "Imagine placing points inside:\n",
        "* A 1D line → points are clearly near or far\n",
        "* A 2D square → points are still close\n",
        "* A 10D cube → almost all points are near the edges, and very far from one another\n",
        "\n",
        "**It Affects KNN**\n",
        "* In low dimensions\n",
        "  * KNN can meaningfully identify nearest neighbors.\n",
        "* In high dimensions\n",
        "  * Distance differences become less significant\n",
        "  * KNN predictions become unreliable\n",
        "  * Model slows down because it has to compute many high-dimensional distances\n",
        "\n",
        "**Summary**\n",
        "\n",
        "|In Low Dimensions\t|In High Dimensions\n",
        "|-||\n",
        "|Distances are meaningful\t|Distances become similar (almost equal)|\n",
        "|Neighbors are useful\t|Neighbors are hard to define|\n",
        "|KNN works well\t|KNN performs poorly|"
      ],
      "metadata": {
        "id": "tiXZAyCvpzpW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q5. How can you choose the best value of K in KNN?\n",
        "**Ans** - 'K' is the number of neighbors weonsider when making a prediction — and picking the right value is critical for balancing:\n",
        "* Bias\n",
        "* Variance\n",
        "\n",
        "**Methods to Find the Best K**\n",
        "1. Trial and Error with Cross-Validation\n",
        "* Split our data into training and validation sets.\n",
        "* Try several values of K.\n",
        "* Calculate accuracy or error on the validation set.\n",
        "* Pick the K with the best validation performance.\n",
        "\n",
        "2. Elbow Method\n",
        "\n",
        "For classification tasks\n",
        "* Plot a graph of error rate / accuracy vs. K values.\n",
        "* Look for the 'elbow point' where the error stops decreasing rapidly or levels off.\n",
        "* Choose K at this turning point — it's usually a good balance.\n",
        "\n",
        "Example:\n",
        "* K=1 → low training error, high variance\n",
        "* K too large → high bias\n",
        "* Sweet spot in between → balanced.\n",
        "\n",
        "3. Leave-One-Out Cross-Validation\n",
        "* Use each data point as a validation set once, and the rest as training.\n",
        "* Calculate error for each K.\n",
        "* Pick K with the lowest average error.\n",
        "* Good for small datasets, but can be slow for large ones.\n",
        "\n",
        "4. Domain Knowledge\n",
        "* Sometimes the nature of our problem might suggest a sensible K:\n",
        "  * Very noisy data → Larger K smooths out the noise.\n",
        "  * Well-separated data → Smaller K preserves local structure.\n",
        "\n",
        "**Summary**\n",
        "\n",
        "|K Value\t|Effect|\n",
        "|-||\n",
        "|Small|Low bias, high variance|\n",
        "|Large|High bias, low variance|\n",
        "|Balanced|Good trade-off between bias and variance|"
      ],
      "metadata": {
        "id": "AA2n2U-ypz8-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q6. What are KD Tree and Ball Tree in KNN?\n",
        "**Ans** - They're both data structures used to speed up nearest neighbor searches in KNN, especially when we have a lot of data points or high dimensions.\n",
        "\n",
        "KNN, by default, is a brute-force algorithm — it checks the distance from our query point to every other point.\n",
        "\n",
        "**KD Tree**\n",
        "* A binary search tree that splits data points along one feature at each level.\n",
        "* It recursively divides the space into nested half-spaces.\n",
        "* It's working:\n",
        "  * Pick a dimension and split the data at the median value.\n",
        "  * Next level, pick the y-axis, split again.\n",
        "  * Alternate splitting by dimension at each level.\n",
        "* Best for:\n",
        "  * Low to moderate dimensional data.\n",
        "  * Faster than brute-force for medium-size problems.\n",
        "\n",
        "**Ball Tree**\n",
        "* A tree-based structure where data points are organized into hyperspherical clusters.\n",
        "* Each node represents a ball.\n",
        "* It hierarchically divides data into smaller balls.\n",
        "\n",
        "* It's working:\n",
        "  * Data points are grouped into balls.\n",
        "  * Each ball has a center and a radius.\n",
        "  * The tree recursively partitions data so that points within a ball are closer to each other than to points in other balls.\n",
        "\n",
        "* Best for:\n",
        "  * Higher dimensional data.\n",
        "  * Non-uniform or clustered data.\n",
        "  * Sometimes faster and more flexible than KD Tree when the data isn't evenly spread.\n",
        "\n",
        "**Comparison**\n",
        "\n",
        "|Feature\t|KD Tree\t|Ball Tree|\n",
        "|-|||\n",
        "|Structure\t|Divides space with axis-aligned splits\t|Divides space with hyperspheres|\n",
        "|Best for\t|Low to moderate dimensions (up to 20-30)\t|Higher dimensions (30+)|\n",
        "|Speed\t|Fast for small-medium low-dimensional data\t|Fast for high-dimensional or unevenly distributed data|\n",
        "|Drawback\t|Struggles in high-dimensional space\t|Slightly more complex structure|\n",
        "\n",
        "**In Practice (with scikit-learn)**\n",
        "\n",
        "In scikit-learn's KNeighborsClassifier or KNeighborsRegressor:"
      ],
      "metadata": {
        "id": "O5fTEt0_p0Q_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "model = KNeighborsClassifier(n_neighbors=5, algorithm='kd_tree')\n",
        "model = KNeighborsClassifier(n_neighbors=5, algorithm='ball_tree')"
      ],
      "metadata": {
        "id": "mUs7uXBXO3eQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q7. When should you use KD Tree vs. Ball Tree?\n",
        "**Ans** - Choosing between KD Tree and Ball Tree really depends on the characteristics of our data.\n",
        "\n",
        "**Use KD Tree**\n",
        "1. Low-Dimensional Data\n",
        "* KD Tree is efficient when our data has fewer dimensions.\n",
        "* It splits data along axis-aligned planes, which is very effective for lower-dimensional spaces.\n",
        "* Example:\n",
        "Data with features like height, weight, age, and income works really well with KD Tree.\n",
        "\n",
        "2. Well-Distributed or Uniform Data\n",
        "* KD Tree performs best when the data is uniformly distributed in the feature space.\n",
        "* If our data points are evenly spread out, the tree will divide the space efficiently.\n",
        "* Example:\n",
        "A dataset of points spread relatively evenly across the 2D plane, like a scatter of cities on a map.\n",
        "\n",
        "3. Faster Search for Smaller Datasets\n",
        "* For small to medium-sized datasets, KD Tree can be faster in practice compared to other methods like brute force.\n",
        "\n",
        "4. When Space is Regular\n",
        "* KD Tree is excellent if the data space doesn't have highly irregular or clustered structures. The axis-aligned splitting works well here.\n",
        "\n",
        "**Use Ball Tree**\n",
        "1. High-Dimensional Data\n",
        "* Ball Tree is more efficient when we have higher dimensions.\n",
        "* In high dimensions, KD Tree starts to struggle because the splitting process becomes less effective. Ball Tree, with its hyperspherical partitions, handles this much better.\n",
        "\n",
        "2. Non-Uniform or Clustered Data\n",
        "* Ball Tree excels when data points are clustered in some regions and sparse in others.\n",
        "* Instead of splitting based on axis-alignment, Ball Tree groups data into balls, making it well-suited for cases where data is naturally grouped.\n",
        "* Example:\n",
        "Data with dense regions and sparse areas, such as image data or geospatial data.\n",
        "\n",
        "3. Irregularly Distributed Data\n",
        "* If the data points are not uniformly distributed, Ball Tree will likely perform better because the tree doesn't split space in a rigid, axis-aligned manner.\n",
        "* Example:\n",
        "Data where there's density variation — for instance, predicting similar items in a recommendation system where user behavior clusters in certain areas.\n",
        "\n",
        "4. Larger Datasets\n",
        "* Ball Tree can scale better when working with larger datasets and high-dimensional spaces, especially when the dataset is not very sparse.\n",
        "\n",
        "**When to Choose Each**\n",
        "\n",
        "|Criteria\t|Use KD Tree\t|Use Ball Tree|\n",
        "|-|||\n",
        "|Data Dimensionality\t|Low dimensions (up to 20-30)\t|High dimensions (30+ or more)|\n",
        "|Data Distribution\t|Uniformly distributed\t|Non-uniform or clustered data|\n",
        "|Data Size\t|Smaller to medium-sized datasets\t|Large datasets, especially with high dimensions|\n",
        "|Performance\t|Fast for small, well-distributed data\t|Better for large or sparse, high-dimensional data|\n",
        "|Use Case\t|Well-distributed 2D or 3D data (e.g., scatter plots)\t|Clusters of data in high-dimensional spaces (e.g., image or text data)|"
      ],
      "metadata": {
        "id": "pfSuxkk0p0jO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q8. What are the disadvantages of KNN?\n",
        "**Ans** - **Disadvantages of K-Nearest Neighbors**\n",
        "1. High Computational Cost\n",
        "* Brute-force distance calculation:\n",
        "KNN requires computing the distance between the query point and every other point in the dataset. This can be very slow for large datasets.\n",
        "  * Time Complexity: O(n) for each prediction.\n",
        "  * As wer dataset grows, predicting becomes slower since it involves searching through all data points for each new query.\n",
        "\n",
        "2. Memory Intensive\n",
        "* Storing the entire dataset:\n",
        "KNN is a lazy learner — it doesn't learn a model beforehand but rather uses the entire dataset during the prediction phase.\n",
        "  * Memory usage increases because the algorithm stores all training data in memory. For large datasets, this can become inefficient and require a lot of RAM.\n",
        "\n",
        "3. Poor Performance with High Dimensions\n",
        "* As the number of dimensions increases, the data becomes sparse, and distances between points become less meaningful.\n",
        "* KNN relies heavily on distances to find neighbors, and in high-dimensional spaces, the distance between points becomes more similar across the entire dataset, making the algorithm less effective.\n",
        "\n",
        "4. Sensitive to Noisy Data and Outliers\n",
        "* Outliers can heavily affect the predictions because KNN doesn't have an internal model to smooth things out.\n",
        "  * For example, if an outlier is close to the query point, it might incorrectly influence the classification or regression.\n",
        "* Noise in the data can also disrupt the neighbor finding process and reduce accuracy.\n",
        "\n",
        "5. Requires Feature Scaling\n",
        "* Feature sensitivity:\n",
        "KNN is highly sensitive to the scale of features.\n",
        "  * Features with larger values can dominate the distance calculations, while features with smaller scales might be ignored.\n",
        "* Solution: we need to scale/normalize our data to ensure fair distance computation.\n",
        "\n",
        "6. Choice of K\n",
        "* Selecting the right K is crucial:\n",
        "  * Small K → Overfitting because the model is too sensitive to noise and outliers.\n",
        "  * Large K → Underfitting because the model may oversmooth and lose local structure in the data.\n",
        "* Finding the optimal value of K can sometimes be difficult, requiring techniques like cross-validation.\n",
        "\n",
        "7. Not Ideal for Real-Time Predictions\n",
        "* Since KNN needs to search the entire dataset for each prediction, it is not suitable for real-time predictions or applications that require fast responses.\n",
        "\n",
        "8. Imbalanced Data Problems\n",
        "* KNN can struggle with imbalanced datasets where some classes are underrepresented.\n",
        "  * The algorithm might tend to favor the majority class, especially if K is too small.\n",
        "  * Solution: Weighted KNN can be used to give more importance to the closer neighbors, or we can balance the data using techniques like oversampling or undersampling.\n",
        "\n",
        "**Disadvantages**\n",
        "\n",
        "|Disadvantage\t|Description|\n",
        "|-||\n",
        "|High Computational Cost\t|Slow for large datasets due to distance calculation.|\n",
        "|Memory Intensive\t|Requires storing the entire dataset in memory.|\n",
        "|Curse of Dimensionality\t|Struggles with high-dimensional data.|\n",
        "|Sensitive to Noise & Outliers\t|Outliers and noise can heavily impact results.|\n",
        "|Requires Feature Scaling\t|Sensitive to feature scales, needing normalization.|\n",
        "|Choice of K\t|Selecting the optimal K value is not straightforward.|\n",
        "|Not Ideal for Real-Time\t|Slow predictions, especially for large datasets.|\n",
        "|Imbalanced Data\t|May be biased towards majority class in unbalanced data.|"
      ],
      "metadata": {
        "id": "PK2is_8ip01N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q9. How does feature scaling affect KNN?\n",
        "**Ans** - Feature scaling is critical in K-Nearest Neighbors because the algorithm heavily relies on measuring the distance between data points to identify their nearest neighbors. If the features are not scaled properly, the distance calculations can be biased, leading to inaccurate predictions.\n",
        "\n",
        "**Feature Scaling Matter in KNN**\n",
        "\n",
        "KNN calculates the distance between the data points to find the closest neighbors. If features are on different scales, features with larger values will dominate the distance calculation, and features with smaller values may have little influence, even though they could be important.\n",
        "\n",
        "**Example:**\n",
        "\n",
        "Imagine we have two features, height and income, with the following values for two data points:\n",
        "* Point A: Height = 160 cm, Income = 50,000 dollars\n",
        "* Point B: Height = 180 cm, Income = 200,000 dollars\n",
        "\n",
        "Without scaling:\n",
        "* Distance in height will be the difference between 160 and 180.\n",
        "* Distance in income will be the difference between 50,000 and 200,000.\n",
        "\n",
        "Since income has much larger values, it will dominate the distance metric, making the difference in height seem less significant. This could mislead the algorithm into focusing more on income than height, even if height is just as important in our problem.\n",
        "\n",
        "**Feature Scaling Affects KNN's Performance**\n",
        "1. Without Scaling:\n",
        "* Features with larger numeric ranges will overshadow other features.\n",
        "* KNN may become biased toward the larger-scale features, and distances may not reflect actual closeness in a meaningful way.\n",
        "\n",
        "2. With Scaling:\n",
        "* Normalization or Standardization puts all features on the same scale.\n",
        "* Equal importance is given to all features, and KNN will treat all features fairly when calculating distances.\n",
        "* This ensures that all features contribute equally to the decision-making process.\n",
        "\n",
        "**Feature Scaling for KNN**\n",
        "1. Min-Max Scaling\n",
        "\n",
        "Min-Max scaling rescales the features so they fall within a specific range, usually between 0 and 1.\n",
        "\n",
        "Formula:\n",
        "\n",
        "    X′ = (X-min(X))/(max(X)-min(X))\n",
        "\n",
        "* Good for: When we need our data in a fixed range.\n",
        "* Drawback: It's sensitive to outliers — extreme values can distort the scaling.\n",
        "\n",
        "2. Z-score Scaling\n",
        "\n",
        "Z-score scaling standardizes the data by subtracting the mean and dividing by the standard deviation. This makes the data have a mean of 0 and standard deviation of 1.\n",
        "\n",
        "Formula:\n",
        "\n",
        "    X′ = (X-μ)/σ​\n",
        "Where:\n",
        "* μ is the mean of the feature,\n",
        "* σ is the standard deviation.\n",
        "* Good for: When the data has outliers or we want the data to be centered around 0.\n",
        "* Drawback: It's more complex, and the data could have extreme values if the distribution is very skewed.\n",
        "\n",
        "**Visual Example of the Effect of Feature Scaling in KNN**\n",
        "\n",
        "Without Scaling:\n",
        "* Feature 1: Age (0-100 years)\n",
        "* Feature 2: Income (10,000-200,000)\n",
        "\n",
        "Without scaling, the distance between two points would be dominated by Income, as the range for income is much larger.\n",
        "\n",
        "**With Scaling:**\n",
        "* After scaling, both Age and Income will be on the same scale. This ensures that both features are given equal weight in the distance calculation, and the nearest neighbors are selected more meaningfully."
      ],
      "metadata": {
        "id": "Vu7V6tdvp1IV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q10. What is PCA (Principal Component Analysis)?\n",
        "**Ans** - PCA is a linear transformation method that:\n",
        "* Converts data into a new coordinate system where the axes represent the directions of maximum variance in the data.\n",
        "* The first principal component captures the most variance, the second one captures the next most, and so on.\n",
        "* It essentially projects data onto fewer dimensions without losing much information.\n",
        "\n",
        "**PCA Working**\n",
        "1. Center the Data :\n",
        "\n",
        "Before applying PCA, we subtract the mean of each feature from the data. This centers the data around the origin, which ensures that PCA doesn't get biased by the scale of features.\n",
        "\n",
        "2. Calculate the Covariance Matrix:\n",
        "* The covariance matrix shows how different features vary with each other.\n",
        "* If we have features like height and weight, the covariance matrix captures the relationship between them.\n",
        "\n",
        "3. Compute Eigenvalues and Eigenvectors:\n",
        "* Eigenvectors are the directions in which the data has the most variance. They represent the principal components.\n",
        "* Eigenvalues represent the magnitude of variance along each eigenvector. The higher the eigenvalue, the more significant the eigenvector.\n",
        "\n",
        "4. Sort Eigenvectors by Eigenvalues:\n",
        "\n",
        "The eigenvectors are sorted by their corresponding eigenvalues in descending order, so the first principal component has the highest variance, and the second principal component has the second-highest variance, and so on.\n",
        "\n",
        "5. Project the Data:\n",
        "\n",
        "The original data is projected onto the top principal components. This results in a new set of coordinates for the data, now expressed in fewer dimensions.\n",
        "\n",
        "**PCA Intuition**\n",
        "\n",
        "Imagine we have data points spread out in a 2D space:\n",
        "* Without PCA, our data might be spread along some diagonal direction in the 2D space.\n",
        "* With PCA, we find the axis that best captures the variance, then project the data along this new axis. This new axis could capture most of the spread of our data.\n",
        "\n",
        "If we then move to 3D and apply PCA again, we can reduce dimensions further by projecting onto just 2 or 1 of the new principal components, still preserving the most significant parts of the data.\n",
        "\n",
        "**Use PCA**\n",
        "1. Dimensionality Reduction:\n",
        "* Reduces the number of features in the data, which can speed up computations and make models more interpretable.\n",
        "* Useful for high-dimensional data like text data or images.\n",
        "\n",
        "2. Improves Performance:\n",
        "* In some cases, PCA can improve the performance of machine learning algorithms by reducing overfitting. It removes noisy features and retains the most informative ones.\n",
        "\n",
        "3. Visualize High-Dimensional Data:\n",
        "* PCA is often used for visualizing high-dimensional data by projecting it to 2D or 3D.\n",
        "\n",
        "4. Multicollinearity Reduction:\n",
        "* PCA helps in reducing multicollinearity, which can improve the performance of models like linear regression.\n",
        "\n",
        "**Applications of PCA:**\n",
        "1. Image Compression:\n",
        "* PCA reduces the number of pixels in an image while preserving its key features.\n",
        "\n",
        "2. Data Visualization:\n",
        "* It helps visualize complex data, reducing it to 2D or 3D for better interpretation.\n",
        "\n",
        "3. Feature Extraction:\n",
        "* In machine learning, PCA is used to extract meaningful features and reduce noise in data.\n",
        "\n",
        "4. Preprocessing:\n",
        "* PCA is often applied as a preprocessing step for machine learning algorithms to improve their performance by eliminating irrelevant features.\n",
        "\n",
        "**PCA in Action**\n",
        "\n",
        "Example: Imagine we have a dataset with 3 features: height, weight, age.\n",
        "* Step 1: Standardize the data.\n",
        "* Step 2: Compute the covariance matrix of the data.\n",
        "* Step 3: Calculate the eigenvalues and eigenvectors of the covariance matrix.\n",
        "* Step 4: Choose the top 2 principal components based on the highest eigenvalues.\n",
        "* Step 5: Project the data onto these 2 components, effectively reducing our 3D data to 2D while preserving the most important variance.\n",
        "\n",
        "**Considerations:**\n",
        "* PCA does not work well with non-linear relationships between features. If the data has non-linear patterns, we might need to look into Non-Linear PCA or other techniques like t-SNE or UMAP.\n",
        "* Interpretability: PCA transforms the features into new components, which can be harder to interpret since the new components are combinations of the original features.\n",
        "* oss of Information: If we reduce to too few components, we might lose some important information, which could hurt model performance."
      ],
      "metadata": {
        "id": "LlJU02Fgp1aW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q11. How does PCA work?\n",
        "**Ans** - Principal Component Analysis works by identifying the most important features of wer data, and then transforming the data into a new set of axes aligned with these components. The goal is to reduce the dimensionality of wer data while retaining as much variance as possible.\n",
        "\n",
        "**PCA Working: Step-by-Step**\n",
        "1. Standardize the Data\n",
        "* The first step in PCA is to center the data by subtracting the mean of each feature. If features have different scales, they should also be standardized to have the same scale.\n",
        "\n",
        "This is important because PCA is sensitive to the variance of features, and features with larger scales would dominate the principal components if the data is not standardized.\n",
        "\n",
        "2. Calculate the Covariance Matrix\n",
        "* Covariance tells us how two features change together. If features are highly correlated, their covariance will be large, meaning that they share a similar pattern of variation.\n",
        "* The covariance matrix is computed to represent the covariance between all pairs of features in the data. For a dataset with n features, the covariance matrix is an n x n matrix where:\n",
        "\n",
        "      Cov(X,Y) = 1N∑(Xi-μX)(Yi-μY)\n",
        "This matrix captures the relationships between all pairs of features in the dataset.\n",
        "\n",
        "3. Calculate the Eigenvalues and Eigenvectors of the Covariance Matrix\n",
        "* Eigenvectors and eigenvalues are central to PCA:\n",
        "  * Eigenvectors determine the directions of the new feature axes.\n",
        "  * Eigenvalues represent the magnitude of variance along the corresponding eigenvector.\n",
        "\n",
        "Mathematically, the eigenvector v and eigenvalue λ satisfy the equation:\n",
        "\n",
        "          Cv = λv\n",
        "where\n",
        "* C is the covariance matrix.\n",
        "\n",
        "Why Eigenvectors and Eigenvalues?\n",
        "\n",
        "Eigenvectors represent the directions of maximum variance in the data, and eigenvalues show how much variance each direction accounts for.\n",
        "\n",
        "4. Sort the Eigenvectors by Eigenvalues\n",
        "* Once we have the eigenvectors and eigenvalues, the next step is to sort them in descending order of the eigenvalues. This step ensures that the most significant principal components are chosen first.\n",
        "* The first principal component corresponds to the eigenvector with the largest eigenvalue and captures the most variance in the data. The second principal component corresponds to the second largest eigenvalue, and so on.\n",
        "\n",
        "5. Choose the Top k Principal Components\n",
        "* After sorting, we can decide how many principal components to keep. Typically, we'll select the top k components based on their eigenvalues. The number of components we choose depends on how much variance we want to retain in the data.\n",
        "* The goal is to reduce dimensionality by selecting just the most important components, while still retaining as much variance as possible.\n",
        "\n",
        "6. Project the Data onto the New Principal Components\n",
        "* Finally, we project the original data onto the top k principal components. This is done by multiplying the original data matrix by the matrix of the selected eigenvectors.\n",
        "\n",
        "The result is a new set of k-dimensional data that is a linear combination of the original features, with the data now described by the most important features.\n",
        "\n",
        "**PCA Visualized**\n",
        "\n",
        "Imagine we have a 2D dataset:\n",
        "* The data might look like a set of points spread out along some diagonal line.\n",
        "* The first principal component would be the line that best fits this spread, capturing the maximum variance.\n",
        "* The second principal component would be perpendicular to PC1 and capture the remaining variance.\n",
        "\n",
        "By projecting the data onto just PC1, we have reduced the 2D data to 1D while retaining the maximum possible variance. If we project onto PC1 and PC2, we're back to the original 2D space but with reduced noise.\n",
        "\n",
        "**PCA Working**\n",
        "* Data Simplification: PCA helps to reduce the number of features by combining correlated features into a smaller set of uncorrelated principal components.\n",
        "* Variance Retention: It helps retain as much information as possible with fewer features, which can improve model performance and visualization.\n",
        "* Noise Reduction: By discarding components with low variance, PCA can filter out noise and make patterns in the data clearer.\n",
        "\n",
        "**Example of PCA with a 3D Dataset**\n",
        "\n",
        "Imagine we have a dataset with 3 features: height, weight, and age. Here's how PCA would reduce this 3D dataset:\n",
        "1. Center the data by subtracting the mean of each feature.\n",
        "2. Calculate the covariance matrix to see how the features correlate.\n",
        "3. Compute the eigenvectors and eigenvalues.\n",
        "4. Choose the top 2 eigenvectors based on the largest eigenvalues.\n",
        "5. Project the original 3D data onto the 2D plane defined by the top 2 principal components.\n",
        "\n",
        "Now, the data is represented in a 2D space that still captures most of the variance of the original 3D data."
      ],
      "metadata": {
        "id": "0u-lHE0bp1sH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q12. What is the geometric intuition behind PCA?\n",
        "**Ans** - The geometric intuition behind Principal Component Analysis is rooted in the idea of finding the directions in the data space that capture the maximum variance or spread of the data. PCA essentially involves finding new axes to represent the data such that the most important patterns in the data are preserved in these new axes.\n",
        "\n",
        "**1. Data as Points in Space**\n",
        "\n",
        "Imagine we have a dataset with two features like height and weight. Each data point can be represented as a point in 2D space:\n",
        "* The x-axis represents height.\n",
        "* The y-axis represents weight.\n",
        "\n",
        "If we have a set of points, the data will be scattered across this 2D space.\n",
        "\n",
        "**2. Finding the \"Best\" Direction**\n",
        "\n",
        "PCA's main task is to find a new direction in the data space where the data points are spread out the most. This direction is called the first principal component.\n",
        "\n",
        "Geometrically:\n",
        "* we want to find a straight line in the 2D space that best fits the distribution of the points.\n",
        "* The goal is to find the line along which the data has the greatest variance, meaning the points are most spread out in that direction.\n",
        "\n",
        "In 2D, this line will be a line of best fit that minimizes the perpendicular distance from the points to the line. This is the direction that captures the maximum variation in the data.\n",
        "\n",
        "Example:\n",
        "* Imagine our data is somewhat diagonal.\n",
        "* The first principal component will be a line diagonal to this data spread, capturing the maximum variance in the data along that direction.\n",
        "\n",
        "**3. Perpendicular Direction**\n",
        "Once we have the first principal component, the next step is to find a second direction that is perpendicular to the first one.\n",
        "* This second direction captures the next largest variance in the data, but it is orthogonal to the first component.\n",
        "* In 2D, the second principal component will be a line perpendicular to the first, and it captures the remaining variation in the data that isn't covered by PC1.\n",
        "\n",
        "**4. Dimensionality Reduction**\n",
        "\n",
        "Once we've found the first and second principal components, we can project our data onto these new axes, effectively reducing the dimensionality of our data:\n",
        "* For example, if we start with 3D data, PCA can reduce it to just 2D or even 1D by focusing on the components that explain the most variance.\n",
        "* In higher dimensions, the process is the same, but the axes we choose are in the higher-dimensional space, and the projection is onto those axes.\n",
        "\n",
        "**5. How the Data is Projected**\n",
        "\n",
        "The actual transformation PCA does is to rotate the original data along the new axes defined by the principal components. This involves two key geometric operations:\n",
        "1. Centering the data: Subtract the mean of each feature to center the data around the origin.\n",
        "2. Projecting the data onto the new axes: After finding the new axes, we project the original data points onto these axes. This projection results in lower-dimensional data but retains as much of the variance as possible.\n",
        "\n",
        "**Geometric Intuition with an Example**\n",
        "\n",
        "Imagine we have a set of data points in a 2D space:\n",
        "* Original axes: The data is spread out in a certain direction, but it's not aligned with the axes.\n",
        "* PCA step: PCA finds the direction of maximum spread, which would be the first principal component.\n",
        "* Transformation: we then project the data onto this new direction, creating a new set of points along the first principal component.\n",
        "* Perpendicular component: PCA also finds the second principal component, which is perpendicular to the first and represents the next largest variance in the data.\n",
        "\n",
        "By rotating the data to align with these new axes, we're compressing the data into a more efficient representation without losing much information.\n",
        "\n",
        "**Geometric Interpretation of Eigenvectors and Eigenvalues**\n",
        "* The eigenvectors represent the directions of the new axes. They are vectors that point in the direction of maximum variance in the data.\n",
        "* The eigenvalues represent the magnitude of variance along each of these new axes. Larger eigenvalues mean that the corresponding eigenvector captures more of the variance, making that component more important.\n",
        "\n",
        "**Visual Example in 2D:**\n",
        "1. Data Points: Suppose we have data that is not aligned with the axes, but has a clear pattern in some diagonal direction.\n",
        "2. First Eigenvector: The first eigenvector will point along the direction of the diagonal, capturing the maximum variance in the data.\n",
        "3. Second Eigenvector: The second eigenvector will point along the perpendicular direction to the first, capturing the remaining variance."
      ],
      "metadata": {
        "id": "rAJzKMppp18u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q13. What is the difference between Feature Selection and Feature Extraction?\n",
        "**Ans** - Feature Selection and Feature Extraction are both techniques used to reduce the number of features in a dataset, but they work in fundamentally different ways.\n",
        "\n",
        "**Feature Selection:**\n",
        "* Feature selection is the process of selecting a subset of the most relevant features from the original set, without transforming the features. we keep the original features but remove irrelevant, redundant, or noisy ones.\n",
        "\n",
        "It's working:\n",
        "* No transformation of the original features occurs.\n",
        "* Features are selected based on certain criteria.\n",
        "* It's about filtering out the less useful features.\n",
        "\n",
        "Methods of Feature Selection:\n",
        "1. Filter Methods: These are independent of any machine learning algorithms and rely on statistical measures to select the most relevant features.\n",
        "  * Example: Using Pearson correlation to remove features that are highly correlated.\n",
        "\n",
        "2. Wrapper Methods: These methods evaluate subsets of features based on the performance of a machine learning model. They try to find the best subset of features by training a model and measuring its performance.\n",
        "  * Example: Recursive Feature Elimination, where a model is trained multiple times and features are removed iteratively to improve performance.\n",
        "\n",
        "3. Embedded Methods: These methods perform feature selection during the model training process. Algorithms like Lasso regression or Decision Trees automatically select important features during model fitting.\n",
        "  * Example: Lasso regression applies L1 regularization and automatically shrinks the coefficients of less important features to zero.\n",
        "\n",
        "**Advantages of Feature Selection:**\n",
        "* Simplicity: The features we keep are the original features, so it's easier to interpret.\n",
        "* Less computation: Reducing the number of features can speed up model training and reduce overfitting.\n",
        "\n",
        "**Disadvantages of Feature Selection:**\n",
        "* Might miss interactions: Feature selection only selects or rejects entire features, so we may miss some relationships or interactions between features that could have been useful.\n",
        "\n",
        "**Feature Extraction:**\n",
        "* Feature extraction is the process of transforming the data into a new set of features, typically by combining or deriving new features that represent the most important information in a reduced dimensional space.\n",
        "\n",
        "It's working:\n",
        "* Transformation happens, and the original features are combined or mapped into new features.\n",
        "* New feature are created, which might not directly resemble the original features.\n",
        "\n",
        "Methods of Feature Extraction:\n",
        "1. Principal Component Analysis: PCA transforms the data into new features called principal components. These components are linear combinations of the original features that capture the maximum variance in the data.\n",
        "  * Example: Reducing 5 features into 2 principal components that capture most of the variance in the data.\n",
        "\n",
        "2. Linear Discriminant Analysis: LDA is used for dimensionality reduction while maintaining class separability. It transforms features to maximize the separation between classes.\n",
        "  * Example: Reducing dimensions while ensuring that different classes in the data remain separable.\n",
        "3. Autoencoders: These are neural networks used for unsupervised learning. The network learns to compress the data into a lower-dimensional representation, and then reconstruct it back to the original features.\n",
        "  * Example: A neural network trained to extract the most important features for image compression.\n",
        "\n",
        "4. t-SNE or UMAP: These are techniques used for nonlinear dimensionality reduction that help in visualizing complex data in 2D or 3D while preserving the data's structure.\n",
        "  * Example: Reducing high-dimensional data to 2D for visualization.\n",
        "\n",
        "**Advantages of Feature Extraction:**\n",
        "* Reduced Dimensionality: Can significantly reduce the number of features while preserving important patterns.\n",
        "* New Features: Can create new features that capture more complex relationships within the data.\n",
        "* Better for Complex Data: Can handle non-linear data better.\n",
        "\n",
        "**Disadvantages of Feature Extraction:**\n",
        "* Loss of Interpretability: The new features may not be easily interpretable, as they are combinations or transformations of the original features.\n",
        "* More computation: Some feature extraction methods can be computationally intensive and require more time.\n",
        "\n",
        "**Differences:**\n",
        "\n",
        "|Aspect\t|Feature Selection\t|Feature Extraction|\n",
        "|-|||\n",
        "|Nature\t|Selects a subset of original features.\t|Creates new features by transforming the original ones.|\n",
        "|Output\t|A smaller set of the original features.\t|A new set of features, usually lower-dimensional.|\n",
        "|Transformation\t|No transformation of the features.\t|The features are transformed into new combinations.|\n",
        "|Interpretability\t|Easy to interpret, as features remain unchanged.\t|Often less interpretable, as new features are derived.|\n",
        "|Computation\t|Typically faster, as it involves selecting features.\t|Can be computationally expensive, especially for complex methods.|\n",
        "|Use Case\t|Works well when we want to keep only the most important features.\t|Works well when we need to reduce dimensionality or extract hidden patterns.|"
      ],
      "metadata": {
        "id": "54X3sWuCp2P-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q14. What are Eigenvalues and Eigenvectors in PCA?\n",
        "**Ans** - Eigenvalues and eigenvectors play a central role in Principal Component Analysis. They are mathematical concepts used to transform the data into a new space, where the data can be analyzed more effectively, particularly when reducing dimensionality.\n",
        "\n",
        "**Eigenvectors:**\n",
        "\n",
        "Eigenvectors represent the directions in the original feature space along which the data is most spread out.\n",
        "* Eigenvectors are vectors that define the axes of the new feature space.\n",
        "* These vectors indicate the directions in which the data has the most variation, or the most important patterns, in the feature space.\n",
        "\n",
        "**Geometric Intuition for Eigenvectors:**\n",
        "* When we perform PCA, we're essentially rotating our data so that the new axes align with the directions of maximum variance.\n",
        "* The first principal component corresponds to the eigenvector with the largest eigenvalue, which is the direction that captures the most variation in the data.\n",
        "* The second principal component corresponds to the second eigenvector, which captures the second-most variation, and so on.\n",
        "\n",
        "**Eigenvalues:**\n",
        "\n",
        "Eigenvalues correspond to the magnitude or amount of variance captured by each eigenvector. They tell us how much variance is captured along each principal component.\n",
        "* Eigenvalues represent how important each principal component is. A larger eigenvalue indicates that the corresponding eigenvector captures more variance, meaning that principal component is more important for explaining the structure of the data.\n",
        "* Smaller eigenvalues indicate less variance, meaning those components are less useful in representing the data's underlying structure.\n",
        "\n",
        "**Geometric Intuition for Eigenvalues:**\n",
        "* The larger the eigenvalue, the more spread out the data is along the direction defined by its corresponding eigenvector.\n",
        "* The eigenvalue reflects how much of the total variance in the dataset is captured by the principal component.\n",
        "\n",
        "**Mathematical Explanation of Eigenvalues and Eigenvectors:**\n",
        "Given a covariance matrix C of a dataset, the eigenvalue λ and eigenvector v satisfy the following equation:\n",
        "\n",
        "    Cv=λv\n",
        "* C is the covariance matrix of the dataset.\n",
        "* v is the eigenvector, representing the direction of maximum variance.\n",
        "* λ is the eigenvalue, representing the amount of variance along that direction.\n",
        "\n",
        "In simpler terms:\n",
        "\n",
        "* Eigenvector v: The direction in which the data has the most spread or variance.\n",
        "* Eigenvalue λ: The amount of variance along the eigenvector v.\n",
        "\n",
        "**Importantance in PCA:**\n",
        "* In PCA, we use the eigenvectors to determine the new axes for our data.\n",
        "* The eigenvalues tell us the importance of these new axes. We can then rank the principal components by their eigenvalues and decide how many components we want to keep for dimensionality reduction.\n",
        "\n",
        "**The Role of Eigenvalues and Eigenvectors in PCA:**\n",
        "1. Data Transformation: PCA finds the eigenvectors of the covariance matrix of the data, which represent the directions in which the data has the most variance. Then, it transforms the data into this new space.\n",
        "2. Variance Explained: The eigenvalues associated with each eigenvector give the amount of variance explained by that principal component. The larger the eigenvalue, the more information the corresponding eigenvector captures.\n",
        "3. Dimensionality Reduction: By sorting the eigenvectors according to their eigenvalues, we can select the most important components that capture the most variance and discard less important ones. This is the key idea behind dimensionality reduction in PCA.\n",
        "\n",
        "**Geometric Example:**\n",
        "\n",
        "Imagine we have a dataset with two features, and we want to perform PCA. Here's a simplified geometric interpretation:\n",
        "1. Data Distribution: our data points may be spread out in a certain direction.\n",
        "2. First Principal Component: The first eigenvector points in the direction that best captures the variance in the data. The corresponding eigenvalue is large because the data is widely spread along this diagonal direction.\n",
        "3. Second Principal Component: The second eigenvector will be perpendicular to the first one and captures the remaining variance. Its corresponding eigenvalue will be smaller.\n",
        "\n",
        "After applying PCA:\n",
        "* We will have a new set of axes, with the first component capturing the most information, and the second component capturing less.\n",
        "* We can project the data onto the first few components, which will reduce dimensionality but retain most of the data's variance."
      ],
      "metadata": {
        "id": "ZgBFnWkHp2ie"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q15. How do we decide the number of components to keep in PCA?\n",
        "**Ans** - Deciding the number of components to keep in Principal Component Analysis is a critical step in dimensionality reduction. The goal is to retain as much variance in the data as possible while reducing the number of features to make the dataset more manageable for modeling and analysis.\n",
        "\n",
        "Some common methods to decide how many principal components to retain:\n",
        "\n",
        "**1. Explained Variance Threshold**\n",
        "\n",
        "One of the most popular methods is to retain enough components to explain a certain percentage of the variance in the data. Typically, we choose a threshold for how much of the total variance we want to retain.\n",
        "* Variance explained by each component: After performing PCA, each principal component has an associated eigenvalue that tells us how much variance it captures from the data.\n",
        "* Cumulative explained variance: The cumulative sum of the explained variance tells us how much variance is captured by the first k components. We can plot the cumulative variance and select the number of components that reaches the desired threshold.\n",
        "\n",
        "Steps:\n",
        "1. Compute explained variance for each component: This can be done by dividing each eigenvalue by the sum of all eigenvalues.\n",
        "2. Plot the cumulative explained variance: This shows the percentage of total variance captured by the first k components.\n",
        "3. Set a threshold: Select the number of components such that the cumulative variance reaches a threshold.\n",
        "\n",
        "Example:\n",
        "* If we want to retain 95% of the variance, look for the smallest number k such that the cumulative variance is ≥ 0.95."
      ],
      "metadata": {
        "id": "b5HuZrHpp21m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "\n",
        "pca = PCA()\n",
        "pca.fit(X)\n",
        "\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "cumulative_variance = np.cumsum(explained_variance)\n",
        "\n",
        "plt.plot(range(1, len(cumulative_variance)+1), cumulative_variance, marker='o')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('Explained Variance vs. Number of Components')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EJ6W_ohtXc_1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* From the plot, we can determine how many components are needed to retain a specific percentage of variance.\n",
        "\n",
        "**2. Scree Plot**\n",
        "\n",
        "A scree plot shows the eigenvalues associated with each principal component. It is a line plot that helps visualize how much variance is explained by each component.\n",
        "* The elbow point of the scree plot often suggests an optimal number of components to retain.\n",
        "* The idea is to retain the components before the \"elbow\" because they explain the most variance, and after that point, additional components contribute less to the variance.\n",
        "\n",
        "**Steps:**\n",
        "1. Plot the eigenvalues of the components.\n",
        "2. Look for the elbow point where the eigenvalues start to decrease more slowly.\n",
        "3. Retain the components before this elbow.\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "IE7OtfsLXh4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(range(1, len(explained_variance)+1), explained_variance, marker='o')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Explained Variance')\n",
        "plt.title('Scree Plot')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9OXNJojJXwzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* The \"elbow\" in the scree plot indicates a point where adding more components yields diminishing returns in terms of explained variance.\n",
        "\n",
        "**3. Kaiser Criterion**\n",
        "\n",
        "This method suggests that we should retain all components with eigenvalues greater than 1. The logic behind this is that an eigenvalue greater than 1 indicates that the principal component explains more variance than an individual feature in the original dataset.\n",
        "\n",
        "**Steps:**\n",
        "1. Calculate the eigenvalues of the covariance matrix.\n",
        "2. Retain all components whose eigenvalue is greater than 1.\n",
        "\n",
        "**Limitations:**\n",
        "* While simple, this rule doesn't always provide the most optimal selection of components, especially in datasets with noise or where features have different scales.\n",
        "\n",
        "**4. Cross-Validation or Model-Based Methods**\n",
        "\n",
        "We can use cross-validation or model-based techniques to determine the optimal number of components based on model performance. For example:\n",
        "* Perform cross-validation with different numbers of principal components.\n",
        "* Evaluate model performance on a downstream task like classification or regression.\n",
        "* Choose the number of components that provides the best performance or the highest trade-off between variance retention and model performance.\n",
        "\n",
        "**Steps:**\n",
        "1. Apply PCA with a varying number of components.\n",
        "2. Evaluate a machine learning model using cross-validation on the transformed data.\n",
        "3. Select the number of components that gives the highest performance.\n",
        "\n",
        "**5. Domain Knowledge and Interpretability**\n",
        "\n",
        "Sometimes the number of components to retain can also be decided based on domain knowledge or the interpretability of the components.\n",
        "* For example, if we are working with image data and we know that most of the important features are captured in the first few principal components, we may choose to retain those.\n",
        "* Similarly, if we're working with text data, we might want to retain enough components to capture the most meaningful patterns but avoid going too high to ensure interpretability."
      ],
      "metadata": {
        "id": "lGX8nYDdX0ok"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q16. Can PCA be used for classification?\n",
        "**Ans** - Principal Component Analysis is primarily a dimensionality reduction technique rather than a direct classification method. However, it can be used as a preprocessing step in classification tasks, and in some cases, it can help improve the performance of classification algorithms.\n",
        "\n",
        "**PCA as a Preprocessing Step for Classification:**\n",
        "1. Reducing Dimensionality:\n",
        "* High-dimensional datasets can suffer from problems like overfitting, high computational cost, and poor generalization. PCA helps by reducing the number of features while retaining most of the variance in the data.\n",
        "* After applying PCA, we transform the data into a lower-dimensional space, which makes it easier for classification models to learn the underlying patterns in the data.\n",
        "\n",
        "2. Improving Model Performance:\n",
        "* Some classification algorithms can benefit from PCA, especially when the original dataset has a lot of noise, redundant features, or correlated features.\n",
        "* By removing less important features, PCA can help the model focus on the most meaningful patterns, improving its accuracy and speed.\n",
        "\n",
        "3. Visualizing Data:\n",
        "* PCA is also commonly used for visualizing high-dimensional data. By projecting data onto the first two or three principal components, we can get an intuitive sense of how well the data can be separated for classification tasks.\n",
        "* This is particularly useful for classification models like SVM or logistic regression, where we can visualize how well the decision boundary separates classes in the reduced feature space.\n",
        "\n",
        "**PCA Works in Classification:**\n",
        "1. Apply PCA to the dataset to reduce dimensionality.\n",
        "2. Train the classifier on the transformed dataset.\n",
        "3. Evaluate the classifier's performance based on its ability to predict class labels.\n",
        "\n",
        "Example Steps:\n",
        "1. Apply PCA to reduce dimensionality.\n",
        "2. Train a classifier on the transformed data.\n",
        "3. Evaluate performance on a test set.\n",
        "\n",
        "Example with Python:"
      ],
      "metadata": {
        "id": "yuh1OyBDp3IG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "clf = SVC(kernel='linear')\n",
        "clf.fit(X_train_pca, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test_pca)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy:.2f}')"
      ],
      "metadata": {
        "id": "HATZHnoOYzvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Explanation:\n",
        "  * PCA is applied to the training data to reduce it to 2 components.\n",
        "  * An SVM classifier is then trained on the reduced dataset.\n",
        "  * The classifier's performance is evaluated using accuracy.\n",
        "\n",
        "**Use PCA in Classification:**\n",
        "1. High Dimensionality:\n",
        "  * PCA is particularly useful when we have many features and suspect that not all features are equally important.\n",
        "  * For example, datasets with thousands of features can benefit from PCA to reduce the feature space.\n",
        "\n",
        "2. Collinearity:\n",
        "  * When features are highly correlated, PCA can help by transforming the features into a set of uncorrelated components, which can improve the performance of classifiers like logistic regression or SVM.\n",
        "\n",
        "3. Noise Reduction:\n",
        "  * If the dataset contains a lot of noise or irrelevant features, PCA can help by removing the components with lower variance that often correspond to noise, leaving only the most informative components.\n",
        "\n",
        "**When Not to Use PCA in Classification:**\n",
        "1. Interpretability:\n",
        "* If we need to interpret the results in terms of the original features, PCA may not be ideal. After PCA, the principal components are linear combinations of the original features, so it's harder to interpret the impact of individual features.\n",
        "\n",
        "2. Small Datasets:\n",
        "* For smaller datasets, PCA may remove important features that could actually help in classification. In such cases, it's better to use classifiers directly on the original data or with minimal dimensionality reduction.\n",
        "\n",
        "3. Loss of Information:\n",
        "* When we reduce dimensions too much, we might lose important information that is needed for classification. The decision on how many components to keep should be carefully chosen based on the explained variance."
      ],
      "metadata": {
        "id": "qwI---UHY4IU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q17. What are the limitations of PCA?\n",
        "**Ans** - Principal Component Analysis is a powerful and widely used technique for dimensionality reduction, but it does have some limitations and drawbacks. These limitations can affect its effectiveness in certain situations.\n",
        "\n",
        "**1. Linear Assumption**\n",
        "* Limitation: PCA assumes that the relationships between features are linear. It identifies the principal components by finding linear combinations of the original features that maximize the variance in the data.\n",
        "* Impact: If the underlying structure of the data is non-linear, PCA may fail to capture important patterns or features of the data. In such cases, non-linear dimensionality reduction techniques may perform better.\n",
        "\n",
        "**2. Sensitivity to Outliers**\n",
        "* Limitation: PCA is sensitive to outliers because it is based on variance, and outliers can significantly increase variance in the data. Since PCA tries to find the directions with the maximum variance, an outlier can distort the directions of the principal components.\n",
        "* Impact: Outliers can lead to a poor or misleading representation of the data in the lower-dimensional space. In some cases, it might result in principal components that do not represent the true structure of the data.\n",
        "* Solution: we can pre-process the data by removing or handling outliers before applying PCA.\n",
        "\n",
        "**3. Difficulty in Interpretation**\n",
        "* Limitation: After performing PCA, the new features are linear combinations of the original features. This can make it difficult to interpret what each principal component represents, especially if the original data contains many features.\n",
        "* Impact: If interpretability of the results is crucial, PCA might not be the best choice because the transformed features don't correspond directly to the original features.\n",
        "* Solution: Techniques like Factor Analysis or Independent Component Analysis might be more suitable when we need both dimensionality reduction and interpretability.\n",
        "\n",
        "**4. Loss of Information**\n",
        "* Limitation: When we reduce the number of dimensions using PCA, we may lose some important information. Even though PCA tries to retain the most important variance in the data, some useful but subtle patterns might get discarded, especially if we reduce the dimensions too much.\n",
        "* Impact: If we retain too few components, we might reduce the variance too much, leading to the loss of key information that could be valuable for tasks like classification or prediction.\n",
        "* Solution: Carefully choose the number of components to retain based on the cumulative explained variance or cross-validation, ensuring that the retained components capture enough variance while minimizing information loss.\n",
        "\n",
        "**5. Assumes Zero Mean Data**\n",
        "* Limitation: PCA requires that the data be centered, meaning each feature should have a mean of zero. If the data is not centered, the results of PCA may be distorted.\n",
        "* Impact: If the data is not preprocessed, the principal components will be shifted, and the results might not be meaningful.\n",
        "* Solution: Always center the data before applying PCA, or use techniques that automatically handle centering.\n",
        "\n",
        "**6. Assumes Features are on the Same Scale**\n",
        "* Limitation: PCA is sensitive to the scale of the features. If the features in wer dataset are on different scales, the principal components will be dominated by the features with larger scales, potentially distorting the results.\n",
        "* Impact: Features with larger numeric values will disproportionately influence the principal components, leading to biased results.\n",
        "* Solution: Standardize or normalize the data so that all features have the same scale before applying PCA.\n",
        "\n",
        "**7. Difficulty in Handling Categorical Data**\n",
        "* Limitation: PCA works best with continuous numerical data. It is not well-suited for datasets that contain categorical variables.\n",
        "* Impact: If we apply PCA directly to categorical data, it won't capture meaningful patterns, as categorical features do not have an inherent numerical relationship.\n",
        "* Solution: If we want to apply PCA to datasets with categorical data, we need to encode categorical variables into numerical values before applying PCA.\n",
        "\n",
        "**8. Computational Complexity for Large Datasets**\n",
        "* Limitation: PCA can be computationally expensive, especially for large datasets with many features and observations. The calculation of the covariance matrix and finding the eigenvectors and eigenvalues can become time-consuming as the size of the dataset increases.\n",
        "* Impact: For datasets with millions of features or observations, performing PCA might take a considerable amount of time and memory.\n",
        "* Solution: For very large datasets, we can use approximate methods for PCA, such as Incremental PCA or Randomized PCA, which scale better with large datasets.\n",
        "\n",
        "**9. No Guarantee of Improved Model Performance**\n",
        "* Limitation: While PCA often helps reduce dimensionality and improve computation speed, reducing dimensionality doesn't always lead to better model performance. In some cases, especially if the data is already well-structured, PCA might actually hurt the performance by discarding relevant features.\n",
        "* Impact: If the principal components don't capture the most important discriminative features for a model, PCA might lead to worse model performance, especially in tasks like classification and regression.\n",
        "* Solution: Always evaluate the performance of a model with and without PCA using cross-validation to ensure that dimensionality reduction leads to better results.\n",
        "\n",
        "**10. Linear Combinations May Not Be Physically Meaningful**\n",
        "* Limitation: The principal components obtained from PCA are linear combinations of the original features. In some domains, these components may not have an intuitive or meaningful physical interpretation, especially in fields like physics or biology.\n",
        "* Impact: If we need to interpret the reduced features in terms of the original features, the results may not be easily interpretable.\n",
        "* Solution: For domain-specific applications, we may need to look for techniques that provide more interpretable results."
      ],
      "metadata": {
        "id": "SuO9ywwap3ZN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q18. How do KNN and PCA complement each other?\n",
        "K-Nearest Neighbors and Principal Component Analysis can work together in complementary ways to improve the performance of machine learning models, especially when dealing with high-dimensional datasets.\n",
        "\n",
        "**1. PCA for Dimensionality Reduction → KNN for Classification**\n",
        "* Problem with high-dimensional data: When we have a high number of features, KNN can become less effective. This is due to the curse of dimensionality, where the distance between points becomes less meaningful as the number of dimensions increases. High-dimensional spaces lead to more sparse data, making it harder for KNN to identify meaningful neighbors.\n",
        "* How PCA helps: PCA reduces the dimensionality of the data by finding new axes that capture the most variance in the data. This results in a lower-dimensional representation of the data, where the most informative features are retained, and less important, redundant, or noisy features are discarded.\n",
        "* Complementary Effect: By reducing the number of dimensions through PCA, the data becomes more compact, and the distance metrics used by KNN become more meaningful. This can improve the performance of KNN, making it faster and more accurate in high-dimensional spaces.\n",
        "\n",
        "**2. Improved Efficiency in KNN Classification**\n",
        "* KNN's computational cost: KNN can be computationally expensive, especially when the number of features is very high because it calculates the distance between the test point and all training points. For large datasets, this can be quite slow.\n",
        "* How PCA helps: By reducing the number of features before applying KNN, PCA reduces the number of distance calculations that KNN has to perform. This speeds up the classification process, as fewer dimensions need to be considered when calculating distances.\n",
        "\n",
        "**3. Better Handling of Noise and Redundancy in KNN**\n",
        "* Noise and redundancy in high dimensions: In high-dimensional data, features can often be highly correlated or noisy, which may confuse KNN. KNN relies heavily on the distance metric, so redundant or noisy features can distort distance calculations and hurt its performance.\n",
        "* How PCA helps: PCA eliminates redundancy by transforming the data into a new set of uncorrelated components. By selecting the components that capture the most variance, PCA removes noise and correlated features, leaving behind a cleaner, more informative representation of the data.\n",
        "* Complementary Effect: This makes the distance calculations in KNN more reliable and meaningful, which can lead to better classification accuracy.\n",
        "\n",
        "**4. Reducing Overfitting in KNN**\n",
        "* Overfitting in KNN: KNN is prone to overfitting, especially when the dataset has many features and the model becomes too sensitive to noise or irrelevant data points.\n",
        "* How PCA helps: By reducing the number of features, PCA helps to simplify the model, making it less likely to overfit the data. Fewer features means fewer parameters to consider, and thus, the model becomes less sensitive to variations that might not generalize well.\n",
        "* Complementary Effect: The reduced feature space provided by PCA can help generalize the KNN model, leading to better performance on unseen data.\n",
        "\n",
        "**5. Visualizing Data for KNN**\n",
        "* High-dimensional data visualization: KNN might struggle with high-dimensional data because it is difficult to visualize and understand the relationships between features in more than three dimensions.\n",
        "* How PCA helps: PCA can reduce the dimensionality to 2 or 3 components, allowing we to visualize the dataset in a lower-dimensional space. This makes it easier to understand the structure of the data and how KNN might classify different data points.\n",
        "* Complementary Effect: Visualizing the data in 2D or 3D space can provide insights into the separability of different classes and guide the choice of K for the KNN model. It also allows for more intuitive analysis and debugging of the KNN classifier.\n",
        "\n",
        "**6. PCA Before KNN for Feature Selection**\n",
        "* Feature selection: In some cases, dimensionality reduction with PCA can serve as a form of feature selection. It helps to select only the most important components, which can be useful in focusing on the most relevant information for KNN classification.\n",
        "* How PCA helps: By applying PCA first, we can focus on the top principal components that capture the most variance in the data. These components are often the most informative, and using them for classification can result in a more efficient and effective KNN model.\n",
        "* Complementary Effect: This combination helps avoid overfitting due to irrelevant or noisy features while ensuring that KNN works with the most valuable information in the dataset.\n",
        "\n",
        "**Example: How PCA and KNN Can Be Used Together**\n",
        "1. Step 1: Apply PCA - Reduce the dimensionality of the data, retaining enough components that explain most of the variance.\n",
        "\n",
        "2. Step 2: Apply KNN - Use the transformed data to train and evaluate the KNN classifier. Since the data is now lower-dimensional and cleaner, the KNN algorithm is more likely to perform well."
      ],
      "metadata": {
        "id": "WNEhTTlQp3rO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q19. How does KNN handle missing values in a dataset?\n",
        "**Ans** - K-Nearest Neighbors does not inherently handle missing values in a dataset. Since KNN relies on calculating the distance between data points to determine neighbors, missing values can cause problems in these distance calculations. However, there are several strategies that can be used to handle missing values when using KNN:\n",
        "\n",
        "**1. Imputation**\n",
        "\n",
        "One of the most common ways to handle missing values in KNN is through imputation, which involves filling in the missing values with estimates based on the rest of the data.\n",
        "* Simple Imputation Techniques:\n",
        "  * Mean/Median Imputation: Replace missing values of a feature with the mean or median value of that feature across all instances.\n",
        "    * For numerical features: Use the mean or median of the available data to fill missing values.\n",
        "    * For categorical features: Use the mode of the available data.\n",
        "* KNN Imputation: Use the KNN algorithm itself to predict and fill the missing values. This works by looking at the K-nearest neighbors and using their values to impute the missing ones. KNN finds similar instances to the one with the missing value, and the missing value is replaced by a weighted average or the most frequent value from the neighbors.\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "uDpWYKUrp38o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import KNNImputer\n",
        "imputer = KNNImputer(n_neighbors=3)\n",
        "X_imputed = imputer.fit_transform(X)"
      ],
      "metadata": {
        "id": "sJMK0_oXbXnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this case, KNNImputer from scikit-learn is used to impute missing values based on the neighbors' values.\n",
        "\n",
        "**2. Removing Instances with Missing Values**\n",
        "* Removing Rows: If the number of missing values is relatively small, we can simply drop the rows containing missing values.\n",
        "  * This works well when the missing data is randomly distributed, and the removal of a few rows does not result in significant data loss.\n",
        "  * However, if many rows have missing values, this approach can lead to a significant reduction in the dataset size, which may affect model performance.\n",
        "\n",
        "Example:"
      ],
      "metadata": {
        "id": "nHzsKx6hbbzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_clean = X.dropna()"
      ],
      "metadata": {
        "id": "_CZZNEr_blA6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Removing Columns: If a feature has a significant proportion of missing values, it may be best to drop the feature entirely. This can be useful if the feature is not crucial for prediction or if imputing the missing values doesn't make sense.\n",
        "\n",
        "**3. Using a Distance Metric That Handles Missing Values**\n",
        "* Some advanced distance metrics can handle missing values while calculating the distance between points.\n",
        "  * For example, we could use a custom distance metric where only the available features are used to calculate the distance between two data points. This way, if one feature is missing for a point, the algorithm ignores that feature when calculating the distance.\n",
        "  * Hamming Distance: For categorical variables, we can compute the Hamming distance, which measures the number of mismatched features between two points. In cases where a feature is missing, it can be treated as a mismatch or ignored.\n",
        "\n",
        "However, this approach can be more complex to implement and may not always lead to better results.\n",
        "\n",
        "**4. Using Nearest Neighbor Search for Missing Value Imputation**\n",
        "* Another advanced approach is to use KNN search algorithms to identify nearest neighbors that have non-missing values for the specific features in question. Then, the missing value is imputed based on the values of those nearest neighbors.\n",
        "  * Local Imputation: For missing values in certain features, we use KNN to look for neighbors that have similar values for the other features. Once we find those neighbors, we impute the missing values based on the corresponding values from the neighbors.\n",
        "\n",
        "**5. Use Models That Handle Missing Values**\n",
        "\n",
        "Some models, including certain tree-based models, can handle missing values natively. These models can often make decisions even when some values are missing, by splitting data in ways that don't require complete information for all features.\n",
        "  * While KNN doesn't handle missing values directly, we could use models like Random Forest or XGBoost for certain types of tasks if we're concerned with how missing data is treated."
      ],
      "metadata": {
        "id": "GaC-Cg8iboML"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q20. What are the key differences between PCA and Linear Discriminant Analysis (LDA)?\n",
        "**Ans** - Principal Component Analysis and Linear Discriminant Analysis are both dimensionality reduction techniques, but they are designed for different purposes and work in fundamentally different ways. Here are the key differences between PCA and LDA:\n",
        "\n",
        "**1. Purpose**\n",
        "* PCA:\n",
        "  * PCA is an unsupervised technique used to reduce the dimensionality of data by finding new directions that capture the maximum variance in the data.\n",
        "  * The primary goal is to capture the maximum variance in the data without considering class labels.\n",
        "* LDA:\n",
        "  * LDA is a supervised technique that seeks to find the directions that maximize the separation between different classes in the data.\n",
        "  * The primary goal is to find linear combinations of features that best separate the classes, i.e., to maximize the class separability.\n",
        "\n",
        "**2. Supervision Type**\n",
        "* PCA: Unsupervised. PCA does not take class labels into account. It only looks at the overall variance in the data.\n",
        "* LDA: Supervised. LDA takes class labels into account during the process of dimensionality reduction and focuses on maximizing the between-class variance while minimizing the within-class variance.\n",
        "\n",
        "**3. Objective**\n",
        "* PCA: The goal is to find the directions that explain the maximum variance in the data, irrespective of the class structure. This can be useful when we just want to reduce the dimensionality of the data without regard to class labels.\n",
        "* LDA: The goal is to find a lower-dimensional representation that maximizes the separation between classes. LDA aims to find a subspace where the between-class variance is maximized and the within-class variance is minimized.\n",
        "\n",
        "**4. Output Components**\n",
        "* PCA: Produces orthogonal components that capture the maximum variance in the data. These components are ordered based on how much variance they explain. The first principal component explains the most variance, the second the second most, and so on.\n",
        "* LDA: Produces discriminant axes that best separate the classes. The number of discriminant components is at most C-1, where C is the number of classes.\n",
        "\n",
        "**5. Data Structure**\n",
        "* PCA: Works on continuous data and does not require any information about the class labels. It is purely driven by the variance in the data.\n",
        "* LDA: Works on labeled data. It requires the classes to be known and tries to find a transformation that maximizes class separability.\n",
        "\n",
        "**6. Assumptions**\n",
        "* PCA:\n",
        "  * Assumes that the directions of maximum variance are the most important features in the data.\n",
        "  * Does not assume anything about the class structure of the data.\n",
        "* LDA:\n",
        "  * Assumes that the data from each class follows a Gaussian distribution and has the same covariance matrix for each class.\n",
        "  * Assumes that the features contribute linearly to the class separability.\n",
        "\n",
        "**7. Use Cases**\n",
        "* PCA:\n",
        "  * Dimensionality reduction for unsupervised learning problems.\n",
        "  * Used in cases where we want to reduce the number of features without considering the class labels, such as data compression or when we have a large number of features in exploratory analysis.\n",
        "* LDA:\n",
        "  * Used for supervised learning problems, especially for classification tasks.\n",
        "  * Used when we want to improve class separability and reduce the dimensionality while preserving the class structure, making it useful for classification problems with labeled data.\n",
        "\n",
        "**8. Performance**\n",
        "* PCA: Can be useful for reducing the dimensionality and speeding up the computation in unsupervised tasks, but it does not guarantee better classification performance, as it ignores the class labels.\n",
        "* LDA: Tends to outperform PCA in classification tasks because it focuses on separating the classes, thus increasing class separability. However, LDA may not work well when the class distributions are not Gaussian or if the assumption of equal class covariances is violated.\n",
        "\n",
        "**9. Example:**\n",
        "* PCA: In a dataset with many features, PCA can be used to reduce the dimensionality of the feature space by finding new, uncorrelated features that capture the most variance. It is commonly used in tasks like data compression, noise reduction, and feature extraction in unsupervised settings.\n",
        "* LDA: In a dataset with labeled classes, LDA could be used to reduce the dimensionality of the feature space while ensuring that the data points from different classes are as far apart as possible. It is commonly used in tasks like classification, where the class labels are crucial.\n",
        "\n",
        "**Differences:**\n",
        "\n",
        "|Aspect\t|PCA\t|LDA|\n",
        "|-|||\n",
        "|Type of Technique\t|Unsupervised\t|Supervised|\n",
        "|Objective\t|Maximize variance (data representation) |Maximize class separability|\n",
        "|Input Data\t|No class labels required\t|Class labels required|\n",
        "|Assumptions\t|Variance-driven, no class info\t|Gaussian distribution, equal covariance|\n",
        "|Output\t|Principal components (max variance)\t|Discriminant axes (max separation)|\n",
        "|Use Case\t|Dimensionality reduction in unsupervised tasks\t|Classification, class separability|\n",
        "|Dimensionality\t|No limit on components, can exceed class number\t|Max C-1 components (C = number of classes)|\n",
        "|Computational Complexity\t|Can be computationally expensive in high-dimensional data\t|Can also be computationally expensive, especially with many classes|"
      ],
      "metadata": {
        "id": "5Ua8-7ilp4N2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Practical"
      ],
      "metadata": {
        "id": "rxViMlqcd44a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q21. Train a KNN Classifier on the Iris dataset and print model accuracy\n",
        "**Ans** - KNN Classifier on the Iris dataset and printing its accuracy."
      ],
      "metadata": {
        "id": "zKCg6NEJp4fe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"KNN Classifier Accuracy on Iris dataset:\", accuracy)"
      ],
      "metadata": {
        "id": "ltrKdDxL2ULM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sample Output**"
      ],
      "metadata": {
        "id": "yz_oRNfJ2XX-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "KNN Classifier Accuracy on Iris dataset: 1.0"
      ],
      "metadata": {
        "id": "jNSnrGF42ePF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q22. Train a KNN Regressor on a synthetic dataset and evaluate using Mean Squared Error (MSE)\n",
        "**Ans** -"
      ],
      "metadata": {
        "id": "jWX1vTIup4vG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X, y = make_regression(n_samples=200, n_features=1, noise=15, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "knn_regressor = KNeighborsRegressor(n_neighbors=5)\n",
        "knn_regressor.fit(X_train, y_train)\n",
        "\n",
        "y_pred = knn_regressor.predict(X_test)\n",
        "\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "print(\"KNN Regressor Mean Squared Error on synthetic dataset:\", mse)"
      ],
      "metadata": {
        "id": "byFRCgIv29Ok"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sample Output**"
      ],
      "metadata": {
        "id": "cz__DXhj3AhO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "KNN Regressor Mean Squared Error on synthetic dataset: 276.54"
      ],
      "metadata": {
        "id": "tYj0wjrR3FyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q23. Train a KNN Classifier using different distance metrics (Euclidean and Manhattan) and compare accuracy\n",
        "**Ans** - KNN Classifier on the Iris dataset using two different distance metrics:\n",
        "* Euclidean distance\n",
        "* Manhattan distance"
      ],
      "metadata": {
        "id": "Ui6k5zsup4_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "knn_euclidean = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=2)\n",
        "knn_euclidean.fit(X_train, y_train)\n",
        "y_pred_euclidean = knn_euclidean.predict(X_test)\n",
        "accuracy_euclidean = accuracy_score(y_test, y_pred_euclidean)\n",
        "\n",
        "knn_manhattan = KNeighborsClassifier(n_neighbors=5, metric='minkowski', p=1)\n",
        "knn_manhattan.fit(X_train, y_train)\n",
        "y_pred_manhattan = knn_manhattan.predict(X_test)\n",
        "accuracy_manhattan = accuracy_score(y_test, y_pred_manhattan)\n",
        "\n",
        "print(\"KNN Classifier Accuracy with Euclidean distance:\", accuracy_euclidean)\n",
        "print(\"KNN Classifier Accuracy with Manhattan distance:\", accuracy_manhattan)"
      ],
      "metadata": {
        "id": "U2Do2ba-3zjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sample Output**"
      ],
      "metadata": {
        "id": "P3g6PZdc32-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "KNN Classifier Accuracy with Euclidean distance: 1.0\n",
        "KNN Classifier Accuracy with Manhattan distance: 1.0"
      ],
      "metadata": {
        "id": "xBvfIINq365E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q24. Train a KNN Classifier with different values of K and visualize decision boundaries\n",
        "**Ans** - K affects KNN's decision boundaries.\n",
        "\n",
        "* Use a simple 2D synthetic classification dataset.\n",
        "* Train KNN classifiers with different K values.\n",
        "* Visualize the decision boundaries for each."
      ],
      "metadata": {
        "id": "ifIhw41yp5RG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "X, y = make_classification(n_samples=300, n_features=2, n_redundant=0, n_clusters_per_class=1, n_classes=3, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "plt.figure(figsize=(18, 5))\n",
        "\n",
        "k_values = [1, 5, 15]\n",
        "\n",
        "colors = ['#FFAAAA', '#AAFFAA', '#AAAAFF']\n",
        "\n",
        "for i, k in enumerate(k_values, 1):\n",
        "    knn = KNeighborsClassifier(n_neighbors=k)\n",
        "    knn.fit(X_train, y_train)\n",
        "\n",
        "    h = .02\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
        "                         np.arange(y_min, y_max, h))\n",
        "    Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.subplot(1, 3, i)\n",
        "    plt.contourf(xx, yy, Z, cmap=plt.cm.coolwarm, alpha=0.4)\n",
        "    plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=plt.cm.coolwarm, edgecolor='k')\n",
        "    plt.title(f'K = {k}')\n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel('Feature 2')\n",
        "\n",
        "plt.tight_lawet()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cdeUoSGK4dKD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* K = 1 : Very flexible, highly sensitive to noise — overfitting.\n",
        "* K = 5 : Smoother, more generalized decision boundary.\n",
        "* K = 15: Even smoother — may underfit if k is too large."
      ],
      "metadata": {
        "id": "E6zfeT5K4gls"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q25. Apply PCA before training a KNN Classifier and compare accuracy with and without PCA\n",
        "**Ans** - Combining PCA and KNN Classifier on a dataset, then comparing classification accuracy\n",
        "* Without PCA\n",
        "* With PCA"
      ],
      "metadata": {
        "id": "eJlP1dasp5h9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "knn_no_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_no_pca.fit(X_train, y_train)\n",
        "y_pred_no_pca = knn_no_pca.predict(X_test)\n",
        "accuracy_no_pca = accuracy_score(y_test, y_pred_no_pca)\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_train_pca = pca.fit_transform(X_train)\n",
        "X_test_pca = pca.transform(X_test)\n",
        "\n",
        "knn_pca = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_pca.fit(X_train_pca, y_train)\n",
        "y_pred_pca = knn_pca.predict(X_test_pca)\n",
        "accuracy_pca = accuracy_score(y_test, y_pred_pca)\n",
        "\n",
        "print(\"KNN Classifier Accuracy without PCA:\", accuracy_no_pca)\n",
        "print(\"KNN Classifier Accuracy with PCA (2 components):\", accuracy_pca)"
      ],
      "metadata": {
        "id": "k5aKawoe5FrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sample Output**"
      ],
      "metadata": {
        "id": "zMOkGKml5Jwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "KNN Classifier Accuracy without PCA: 1.0\n",
        "KNN Classifier Accuracy with PCA (2 components): 0.9667"
      ],
      "metadata": {
        "id": "dRlky2BY5Ogw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Without PCA → Uses all 4 original features, gets full accuracy.\n",
        "* With PCA → Small drop in accuracy, but dimensionality is reduced by 50%, often worth it when dealing with high-dimensional, noisy, or computationally expensive data."
      ],
      "metadata": {
        "id": "wHNn1pjD5TKc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q28. Perform Hyperparameter Tuning on a KNN Classifier using GridSearchCV."
      ],
      "metadata": {
        "id": "PY5hDBvQp5yV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "knn = KNeighborsClassifier()\n",
        "\n",
        "param_grid = {\n",
        "    'n_neighbors': [1, 3, 5, 7, 9, 11],\n",
        "    'p': [1, 2]\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(knn, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "best_params = grid_search.best_params_\n",
        "best_score = grid_search.best_score_\n",
        "\n",
        "best_knn = grid_search.best_estimator_\n",
        "y_pred = best_knn.predict(X_test)\n",
        "test_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best Hyperparameters:\", best_params)\n",
        "print(\"Best Cross-Validation Accuracy:\", best_score)\n",
        "print(\"Test Set Accuracy with Best Model:\", test_accuracy)"
      ],
      "metadata": {
        "id": "dUo79Ugo5sY0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sample Output**"
      ],
      "metadata": {
        "id": "oZauYEOB5v3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Best Hyperparameters: {'n_neighbors': 7, 'p': 2}\n",
        "Best Cross-Validation Accuracy: 0.9667\n",
        "Test Set Accuracy with Best Model: 1.0"
      ],
      "metadata": {
        "id": "pYmL8EQO51J8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q29. Train a KNN Classifier and check the number of misclassified samples."
      ],
      "metadata": {
        "id": "DCZFsQw-p6DW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import numpy as np\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "misclassified_count = np.sum(y_test != y_pred)\n",
        "\n",
        "print(\"Number of misclassified samples:\", misclassified_count)"
      ],
      "metadata": {
        "id": "9XSDgYm56XFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sample Output**"
      ],
      "metadata": {
        "id": "zgGg9eWr6bQk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Number of misclassified samples: 1"
      ],
      "metadata": {
        "id": "-zrQixcy6fec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q30. Train a PCA model and visualize the cumulative explained variance."
      ],
      "metadata": {
        "id": "qp3Pj_YTp6WO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "\n",
        "pca = PCA()\n",
        "pca.fit(X)\n",
        "\n",
        "cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='--', color='b')\n",
        "plt.xlabel('Number of Principal Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('PCA - Cumulative Explained Variance')\n",
        "plt.grid(True)\n",
        "plt.axhline(y=0.95, color='r', linestyle='-')\n",
        "plt.text(1.5, 0.96, '95% threshold', color='r')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "2LBFNTN06xxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Fits a PCA model to the Iris dataset.\n",
        "* Calculates cumulative variance explained by adding up the variance ratio of each principal component.\n",
        "* Plots it so we can see how much variance is captured as we add more components.\n",
        "* Adds a 95% threshold line to see how many components are needed to capture most of the variance."
      ],
      "metadata": {
        "id": "50LgQWY366n8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q31. Train a KNN Classifier using different values of the weights parameter (uniform vs. distance) and compare accuracy\n",
        "**Ans** - the weights parameter in KNN controls how neighbors contribute to the final prediction:\n",
        "* uniform → all neighbors contribute equally\n",
        "* distance → closer neighbors have more influence\n",
        "\n",
        "Let's compare them side by side on the Iris dataset."
      ],
      "metadata": {
        "id": "C9pUtBAXp6nQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "knn_uniform = KNeighborsClassifier(n_neighbors=5, weights='uniform')\n",
        "knn_uniform.fit(X_train, y_train)\n",
        "y_pred_uniform = knn_uniform.predict(X_test)\n",
        "accuracy_uniform = accuracy_score(y_test, y_pred_uniform)\n",
        "\n",
        "knn_distance = KNeighborsClassifier(n_neighbors=5, weights='distance')\n",
        "knn_distance.fit(X_train, y_train)\n",
        "y_pred_distance = knn_distance.predict(X_test)\n",
        "accuracy_distance = accuracy_score(y_test, y_pred_distance)\n",
        "\n",
        "print(\"KNN Classifier Accuracy with uniform weights:\", accuracy_uniform)\n",
        "print(\"KNN Classifier Accuracy with distance weights:\", accuracy_distance)"
      ],
      "metadata": {
        "id": "hCzMv6T97ZOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Sample Output**"
      ],
      "metadata": {
        "id": "b2-wdfr27cWD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "KNN Classifier Accuracy with uniform weights: 1.0\n",
        "KNN Classifier Accuracy with distance weights: 1.0"
      ],
      "metadata": {
        "id": "ojPeKHM47gTE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation:**\n",
        "* On clean, small datasets like Iris, both methods often perform equally well.\n",
        "* On noisier or imbalanced datasets, distance weighting often improves performance by reducing the influence of distant, potentially misleading neighbors."
      ],
      "metadata": {
        "id": "SmPHyUxk7iqL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q32. Train a KNN Regressor and analyze the effect of different K values on performance\n",
        "**Ans** -\n",
        "* Training a KNN Regressor on a synthetic regression dataset\n",
        "* Testing several values of k\n",
        "* Evaluating performance using Mean Squared Error\n",
        "* Plotting MSE vs. k to visualize the effect"
      ],
      "metadata": {
        "id": "7d1A7E4yp7O2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "X, y = make_regression(n_samples=300, n_features=1, noise=20, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "k_values = range(1, 21)\n",
        "mse_values = []\n",
        "\n",
        "for k in k_values:\n",
        "    knn_regressor = KNeighborsRegressor(n_neighbors=k)\n",
        "    knn_regressor.fit(X_train, y_train)\n",
        "    y_pred = knn_regressor.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mse_values.append(mse)\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.plot(k_values, mse_values, marker='o', linestyle='--', color='blue')\n",
        "plt.xlabel('Number of Neighbors (k)')\n",
        "plt.ylabel('Mean Squared Error (MSE)')\n",
        "plt.title('Effect of K on KNN Regressor Performance')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Y5ZMV-2XvN9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Small k values (like 1-3) → Very flexible (low bias, high variance) → can overfit\n",
        "* Large k values (like 15-20) → Smoother, more generalized predictions → can underfit\n",
        "* There’s typically a sweet spot (often somewhere around 5-10) where MSE is minimized."
      ],
      "metadata": {
        "id": "bUOxfFLLvRvR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q33. Implement KNN Imputation for handling missing values in a dataset\n",
        "**Ans** - KNN Imputation is an effective method for filling missing values in a dataset by using the K-Nearest Neighbors algorithm to predict missing values based on the nearest available neighbors.\n",
        "\n",
        "Implementation of KNN imputation using the KNNImputer class from sklearn.impute."
      ],
      "metadata": {
        "id": "H5EGitshp7ee"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import KNNImputer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "data = {\n",
        "    'Feature1': [1, 2, np.nan, 4, 5],\n",
        "    'Feature2': [5, np.nan, 7, 8, 10],\n",
        "    'Feature3': [10, 20, 30, np.nan, 50]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "print(\"Original dataset with missing values:\")\n",
        "print(df)\n",
        "\n",
        "imputer = KNNImputer(n_neighbors=2)\n",
        "\n",
        "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
        "\n",
        "print(\"\\nDataset after KNN Imputation:\")\n",
        "print(df_imputed)"
      ],
      "metadata": {
        "id": "8KuoEN71vswB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Original Dataset: We have missing values represented by np.nan.\n",
        "* KNNImputer: We’re filling the missing values using the 2 nearest neighbors (we can adjust n_neighbors).\n",
        "* Imputed Dataset: The missing values are replaced with predictions based on the closest neighbors.\n",
        "\n",
        "**Sample Output:**"
      ],
      "metadata": {
        "id": "88t95ku1v4x9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Original dataset with missing values:\n",
        "   Feature1  Feature2  Feature3\n",
        "0       1.0       5.0      10.0\n",
        "1       2.0       NaN      20.0\n",
        "2       NaN       7.0      30.0\n",
        "3       4.0       8.0       NaN\n",
        "4       5.0      10.0      50.0\n",
        "\n",
        "Dataset after KNN Imputation:\n",
        "   Feature1  Feature2  Feature3\n",
        "0       1.0       5.0      10.0\n",
        "1       2.0       7.5      20.0\n",
        "2       3.0       7.0      30.0\n",
        "3       4.0       8.0      40.0\n",
        "4       5.0      10.0      50.0"
      ],
      "metadata": {
        "id": "6STZeM-2yJ_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "* Feature1: The missing value in row 2 is imputed as the average of its 2 nearest neighbors (rows 1 and 3).\n",
        "* Feature2: The missing value in row 1 is imputed based on neighbors in rows 0 and 2.\n",
        "* Feature3: The missing value in row 3 is imputed using rows 2 and 4."
      ],
      "metadata": {
        "id": "Qwl4RZrMyOHI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q34. Train a PCA model and visualize the data projection onto the first two principal components\n",
        "**Ans** - PCA on the Iris dataset and visualize how the data looks when projected onto the first two principal components."
      ],
      "metadata": {
        "id": "X9JLxOzkp7uw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.decomposition import PCA\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "\n",
        "X_pca = pca.fit_transform(X)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', edgecolor='k', s=100)\n",
        "\n",
        "plt.title('PCA - Projection onto First Two Principal Components')\n",
        "plt.xlabel('First Principal Component')\n",
        "plt.ylabel('Second Principal Component')\n",
        "plt.colorbar(label='Species')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "b4C2CLegyxHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* PCA reduces the original 4-dimensional data into 2 principal components.\n",
        "* The scatter plot shows how the Iris dataset points are spread out in the new 2D space, with points colored according to their species.\n",
        "\n",
        "**Interpretation:**\n",
        "* The data points are grouped in different clusters (representing different Iris species).\n",
        "* The plot helps us understand how well the 2 principal components capture the underlying structure of the dataset."
      ],
      "metadata": {
        "id": "wwefchkvy3BP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q35. Train a KNN Classifier using the KD Tree and Ball Tree algorithms and compare performance\n",
        "**Ans** - KNN Classifier using both the KD Tree and Ball Tree algorithms and compare their performance. These two methods differ in how they organize and search for nearest neighbors, especially when dealing with higher-dimensional data.\n",
        "\n",
        "We'll use the Iris dataset for simplicity and compare:\n",
        "* KD Tree: Efficient for lower-dimensional data.\n",
        "* Ball Tree: More efficient for higher-dimensional data."
      ],
      "metadata": {
        "id": "f6pLh7Yqp7_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "knn_kd_tree = KNeighborsClassifier(n_neighbors=5, algorithm='kd_tree')\n",
        "knn_kd_tree.fit(X_train, y_train)\n",
        "y_pred_kd_tree = knn_kd_tree.predict(X_test)\n",
        "accuracy_kd_tree = accuracy_score(y_test, y_pred_kd_tree)\n",
        "\n",
        "knn_ball_tree = KNeighborsClassifier(n_neighbors=5, algorithm='ball_tree')\n",
        "knn_ball_tree.fit(X_train, y_train)\n",
        "y_pred_ball_tree = knn_ball_tree.predict(X_test)\n",
        "accuracy_ball_tree = accuracy_score(y_test, y_pred_ball_tree)\n",
        "\n",
        "print(f\"Accuracy with KD Tree: {accuracy_kd_tree:.4f}\")\n",
        "print(f\"Accuracy with Ball Tree: {accuracy_ball_tree:.4f}\")"
      ],
      "metadata": {
        "id": "5ZvEpW1-zdqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* KNeighborsClassifier: We train KNN classifiers using both the KD Tree and Ball Tree algorithms.\n",
        "* Comparison: After training, we compare the performance using accuracy score.\n",
        "\n",
        "**Sample Output:**"
      ],
      "metadata": {
        "id": "VOujrY4OzhC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Accuracy with KD Tree: 1.0000\n",
        "Accuracy with Ball Tree: 1.0000"
      ],
      "metadata": {
        "id": "MZgc-S00zqBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation:**\n",
        "* Both KD Tree and Ball Tree may show similar performance on small datasets like Iris.\n",
        "* The KD Tree is more efficient in lower dimensions, while Ball Tree handles higher dimensions better.\n",
        "* If we're dealing with higher-dimensional datasets, Ball Tree is likely to perform better or at least be more efficient computationally."
      ],
      "metadata": {
        "id": "KGyz2_v_ztDB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q36. Train a PCA model on a high-dimensional dataset and visualize the Scree plot\n",
        "**Ans** - PCA model on a high-dimensional dataset and visualize the Scree plot, which shows the explained variance of each principal component. The Scree plot helps us to decide how many components to keep based on the explained variance."
      ],
      "metadata": {
        "id": "fZBQSMRjp8QG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "from sklearn.datasets import make_classification\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "X, y = make_classification(n_samples=300, n_features=100, random_state=42)\n",
        "\n",
        "pca = PCA()\n",
        "pca.fit(X)\n",
        "\n",
        "explained_variance = pca.explained_variance_ratio_\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(range(1, len(explained_variance) + 1), explained_variance, marker='o', linestyle='--', color='b')\n",
        "plt.title('Scree Plot')\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7B7HscZX0Vox"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Synthetic Dataset: We generate a high-dimensional dataset with 100 features using make_classification.\n",
        "* PCA: We apply PCA to reduce the dimensions and obtain the explained variance ratio for each principal component.\n",
        "* Scree Plot: We plot the explained variance ratio for each component.\n",
        "\n",
        "**Sample Output:**\n",
        "* The plot will display a declining curve showing the explained variance of each component.\n",
        "* we'll see that few components explain most of the variance, which helps in dimensionality reduction.\n",
        "\n",
        "**Interpretation:**\n",
        "* The Scree plot visually represents the \"elbow\" point — the point where the variance explained by each additional component starts to drop off significantly.\n",
        "* The first few principal components capture most of the variance, while the rest contribute less and less."
      ],
      "metadata": {
        "id": "eZv8Ddr-0Zn3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q37. Train a KNN Classifier and evaluate performance using Precision, Recall, and F1-Score\n",
        "**Ans** - K-Nearest Neighbors classifier and evaluate its performance using Precision, Recall, and F1-Score in Python with scikit-learn. Here's a clean, well-commented implementation we can run:"
      ],
      "metadata": {
        "id": "70uD-H4Wp8ie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=3)\n",
        "\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "precision = precision_score(y_test, y_pred, average='macro')\n",
        "recall = recall_score(y_test, y_pred, average='macro')\n",
        "f1 = f1_score(y_test, y_pred, average='macro')\n",
        "\n",
        "print(f'Precision: {precision:.2f}')\n",
        "print(f'Recall: {recall:.2f}')\n",
        "print(f'F1-Score: {f1:.2f}')\n",
        "\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "f8MyIZhV4BNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example Output:**"
      ],
      "metadata": {
        "id": "KkIj1zcC0SGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Precision: 1.00\n",
        "Recall: 1.00\n",
        "F1-Score: 1.00\n",
        "\n",
        "Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       1.00      1.00      1.00        10\n",
        "           1       1.00      1.00      1.00         9\n",
        "           2       1.00      1.00      1.00        11\n",
        "\n",
        "    accuracy                           1.00        30\n",
        "   macro avg       1.00      1.00      1.00        30\n",
        "weighted avg       1.00      1.00      1.00        30"
      ],
      "metadata": {
        "id": "ZdEAXMRP0Zzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "* Precision: How many of the predicted positives are actual positives.\n",
        "* Recall: How many of the actual positives are captured by the model.\n",
        "* F1-Score: Harmonic mean of precision and recall."
      ],
      "metadata": {
        "id": "pa7CHWLY0dQa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q38. Train a PCA model and analyze the effect of different numbers of components on accuracy"
      ],
      "metadata": {
        "id": "qVYlLigPp9Xu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans** - PCA (Principal Component Analysis) is a powerful tool for dimensionality reduction, and it’s super insightful to see how the number of components affects classification accuracy.\n",
        "\n",
        "Let’s walk through a clean, reproducible example using PCA + KNN (since we just worked with KNN) — and analyze how accuracy changes as we reduce the number of components."
      ],
      "metadata": {
        "id": "8g99Zi3oXQUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "accuracies = []\n",
        "\n",
        "for n_components in range(1, X.shape[1] + 1):\n",
        "\n",
        "    pca = PCA(n_components=n_components)\n",
        "    X_train_pca = pca.fit_transform(X_train)\n",
        "    X_test_pca = pca.transform(X_test)\n",
        "\n",
        "    knn = KNeighborsClassifier(n_neighbors=3)\n",
        "    knn.fit(X_train_pca, y_train)\n",
        "\n",
        "    y_pred = knn.predict(X_test_pca)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "for i, acc in enumerate(accuracies, start=1):\n",
        "    print(f'Components: {i}, Accuracy: {acc:.2f}')\n",
        "    plt.figure(figsize=(8,5))\n",
        "plt.plot(range(1, X.shape[1] + 1), accuracies, marker='o')\n",
        "plt.title('Effect of PCA Components on KNN Accuracy')\n",
        "plt.xlabel('Number of PCA Components')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(True)\n",
        "plt.xticks(range(1, X.shape[1] + 1))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nAeIjyB13Y9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example Output**"
      ],
      "metadata": {
        "id": "R6CeoxyO4Bpq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Components: 1, Accuracy: 0.57\n",
        "Components: 2, Accuracy: 0.97\n",
        "Components: 3, Accuracy: 1.00\n",
        "Components: 4, Accuracy: 1.00"
      ],
      "metadata": {
        "id": "APgXcW8f4G4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation:**\n",
        "* With 1 component → big drop in accuracy.\n",
        "* With 2 components → already very good (~97%).\n",
        "* dding more components (3 or 4) brings us to perfect classification.\n",
        "* This shows PCA can reduce dimensionality (from 4D to 2D) without much loss in accuracy — and often helps with faster, more efficient models."
      ],
      "metadata": {
        "id": "QGkiNmeb4Klp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q39. Train a KNN Classifier with different leaf_size values and compare accuracy."
      ],
      "metadata": {
        "id": "0Wb-iNqRp9nm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans** - leaf_size is an interesting hyperparameter in KNN that affects the speed of the BallTree or KDTree used internally by the algorithm. It usually doesn’t affect accuracy much, but it can influence performance and computation time — and it’s worth experimenting to confirm this.\n",
        "\n",
        "Let’s set up a clean experiment where we train a KNN classifier on the Iris dataset while varying leaf_size, and then compare accuracies."
      ],
      "metadata": {
        "id": "08fKUtQGXqbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "leaf_sizes = range(5, 60, 5)\n",
        "accuracies = []\n",
        "\n",
        "for leaf_size in leaf_sizes:\n",
        "\n",
        "    knn = KNeighborsClassifier(n_neighbors=3, leaf_size=leaf_size)\n",
        "    knn.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = knn.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracies.append(accuracy)\n",
        "\n",
        "for ls, acc in zip(leaf_sizes, accuracies):\n",
        "    print(f'Leaf Size: {ls}, Accuracy: {acc:.2f}')\n",
        "    plt.figure(figsize=(8,5))\n",
        "plt.plot(leaf_sizes, accuracies, marker='o')\n",
        "plt.title('Effect of leaf_size on KNN Accuracy')\n",
        "plt.xlabel('Leaf Size')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vyNwiOtt4iUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example Output:**"
      ],
      "metadata": {
        "id": "vGSbCBk-5lWJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Leaf Size: 5, Accuracy: 1.00\n",
        "Leaf Size: 10, Accuracy: 1.00\n",
        "Leaf Size: 15, Accuracy: 1.00\n",
        "Leaf Size: 20, Accuracy: 1.00\n",
        "Leaf Size: 25, Accuracy: 1.00\n",
        "Leaf Size: 30, Accuracy: 1.00\n",
        "Leaf Size: 35, Accuracy: 1.00\n",
        "Leaf Size: 40, Accuracy: 1.00\n",
        "Leaf Size: 45, Accuracy: 1.00\n",
        "Leaf Size: 50, Accuracy: 1.00\n",
        "Leaf Size: 55, Accuracy: 1.00"
      ],
      "metadata": {
        "id": "rCPKzVst5p-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation:**\n",
        "* In this case, the Iris dataset is very simple and clean — so changing leaf_size doesn't affect accuracy at all.\n",
        "* In larger, noisier, or more high-dimensional datasets, leaf_size can influence:\n",
        "  * Query speed (faster or slower lookup)\n",
        "  * Memory usage\n",
        "  * Sometimes very slight accuracy differences\n",
        "\n",
        "leaf_size is more about computational efficiency than model accuracy."
      ],
      "metadata": {
        "id": "b0jg2HoH5syC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q40. Train a PCA model and visualize how data points are transformed before and after PCA"
      ],
      "metadata": {
        "id": "7QJ170l-p97v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans** - visualizing data before and after PCA really makes it clear how PCA works by projecting high-dimensional data into a lower-dimensional space while preserving as much variance as possible.\n",
        "\n",
        "Let’s do this step by step using the Iris dataset and project it to 2 principal components so we can easily plot it."
      ],
      "metadata": {
        "id": "Bg3TK5_YYAJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "target_names = iris.target_names\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "for target, color in zip([0, 1, 2], ['r', 'g', 'b']):\n",
        "    plt.scatter(X[y == target, 0], X[y == target, 1], label=target_names[target], c=color)\n",
        "plt.title('Original Data (First 2 Features)')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.legend()\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "for target, color in zip([0, 1, 2], ['r', 'g', 'b']):\n",
        "    plt.scatter(X_pca[y == target, 0], X_pca[y == target, 1], label=target_names[target], c=color)\n",
        "plt.title('Data After PCA (2 Components)')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_lawet()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N5IKrZ3Z6Ixx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Left plot (Before PCA) — Raw data using the first two features (may not separate classes well)\n",
        "* Right plot (After PCA) — Data transformed into the new PCA space (usually better separation since it captures the directions of maximum variance)\n",
        "\n",
        "**Interpretation:**\n",
        "* PCA rotates the original axes into a new space where:\n",
        "  * PC1 (Principal Component 1) captures the most variance.\n",
        "  * PC2 captures the second most, orthogonal to PC1.\n",
        "* we’ll notice much better clustering and separation in the PCA-transformed space."
      ],
      "metadata": {
        "id": "2dIUEP7f78Nh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q41. Train a KNN Classifier on a real-world dataset (Wine dataset) and print classification report"
      ],
      "metadata": {
        "id": "jNnb_-1Fp-VH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans** - Wine dataset is a classic real-world dataset that works great with classification algorithms like KNN.\n",
        "\n",
        "Let’s walk through a clean, practical example where we:\n",
        "* Train a KNN Classifier on the Wine dataset\n",
        "* Predict on the test set\n",
        "* Print a detailed classification report (Precision, Recall, F1-Score, Support)"
      ],
      "metadata": {
        "id": "AtwEORrvYX9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred, target_names=wine.target_names))"
      ],
      "metadata": {
        "id": "BIBnfXTJ8T2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example Output:**"
      ],
      "metadata": {
        "id": "SkTbuQrk83Np"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Classification Report:\n",
        "\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "     class_0       0.94      1.00      0.97        12\n",
        "     class_1       0.89      0.89      0.89         9\n",
        "     class_2       1.00      0.91      0.95        11\n",
        "\n",
        "    accuracy                           0.94        32\n",
        "   macro avg       0.94      0.93      0.94        32\n",
        "weighted avg       0.94      0.94      0.94        32"
      ],
      "metadata": {
        "id": "OieswQvV88ew"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation:**\n",
        "* Precision: How many predicted samples for a class are correct.\n",
        "* Recall: How many actual samples for a class are correctly predicted.\n",
        "* F1-score: Harmonic mean of precision and recall.\n",
        "* Support: Number of actual occurrences of each class in the test set.\n",
        "\n",
        "Overall Accuracy: 94% in this case — great for a simple KNN without any tuning."
      ],
      "metadata": {
        "id": "LSgk_vBy8_H_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q42. Train a KNN Regressor and analyze the effect of different distance metrics on prediction error"
      ],
      "metadata": {
        "id": "EGK8P5r4p-uH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans** - Exploring how different distance metrics affect a KNN Regressor's performance is a super insightful exercise. KNN regression predicts a continuous value by averaging the values of its nearest neighbors, and the choice of distance metric can influence which neighbors it considers \"nearest.\"\n",
        "\n",
        "Let's set this up cleanly and analyze the prediction error with different distance metrics.\n",
        "\n",
        "**How We'll Do It:**\n",
        "* Use the Boston Housing dataset (classic regression dataset — now replaced by fetch_california_housing, so we'll use that)\n",
        "* Train KNNRegressor with different distance metrics:\n",
        "  * Euclidean (p=2)\n",
        "  * Manhattan (p=1)\n",
        "  * Chebyshev (infinite norm)\n",
        "* Compare prediction errors using Mean Squared Error (MSE)."
      ],
      "metadata": {
        "id": "15Y7TywnYx6P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X = housing.data\n",
        "y = housing.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "distance_metrics = {\n",
        "    'Euclidean (p=2)': 2,\n",
        "    'Manhattan (p=1)': 1,\n",
        "    'Chebyshev (p=inf)': np.inf\n",
        "}\n",
        "\n",
        "mse_scores = {}\n",
        "\n",
        "for name, p in distance_metrics.items():\n",
        "    knn = KNeighborsRegressor(n_neighbors=5, p=p)\n",
        "    knn.fit(X_train, y_train)\n",
        "    y_pred = knn.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    mse_scores[name] = mse\n",
        "    print(f'{name} MSE: {mse:.3f}')\n",
        "    plt.figure(figsize=(8,5))\n",
        "plt.bar(mse_scores.keys(), mse_scores.values(), color=['blue', 'green', 'red'])\n",
        "plt.title('Effect of Distance Metric on KNN Regression Error')\n",
        "plt.ylabel('Mean Squared Error (MSE)')\n",
        "plt.xticks(rotation=20)\n",
        "plt.grid(axis='y')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mNi3phY99dk4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example Output:**"
      ],
      "metadata": {
        "id": "6ssfWs2U-MdI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Euclidean (p=2) MSE: 0.544\n",
        "Manhattan (p=1) MSE: 0.530\n",
        "Chebyshev (p=inf) MSE: 0.610"
      ],
      "metadata": {
        "id": "Hgj05SCK-QGY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "📊 Interpretation:\n",
        "* Manhattan (p=1) performed slightly better here with the lowest MSE.\n",
        "* Chebyshev (p=inf) was the worst — this makes sense since it only considers the largest difference along any dimension.\n",
        "* Euclidean (p=2) was close but slightly higher than Manhattan.."
      ],
      "metadata": {
        "id": "CJ-VyJ67-STg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q43. Train a KNN Classifier and evaluate using ROC-AUC score"
      ],
      "metadata": {
        "id": "UycvWSG-p-_W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans** - using ROC-AUC is a powerful way to evaluate a classifier, especially when we care about the trade-off between True Positive Rate (TPR) and False Positive Rate (FPR).\n",
        "\n",
        "Since ROC-AUC works for binary classification problems, we'll need to either:\n",
        "\n",
        "Use a binary dataset\n",
        "or\n",
        "\n",
        "Convert a multi-class dataset (like Iris or Wine) to binary by selecting only two classes.\n",
        "\n",
        "Let’s go with the Wine dataset, and convert it to a binary classification problem (e.g., classifying class_0 vs others). Then we'll train a KNN Classifier and compute the ROC-AUC score."
      ],
      "metadata": {
        "id": "pND6NujT2KID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as npfrom sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, auc\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "y_binary = (y == 0).astype(int)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_binary, test_size=0.2, random_state=42)\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "y_prob = knn.predict_proba(X_test)[:, 1]\n",
        "\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_prob)\n",
        "\n",
        "roc_auc = auc(fpr, tpr)\n",
        "print(f'ROC-AUC Score: {roc_auc:.3f}')\n",
        "\n",
        "plt.figure(figsize=(7, 5))\n",
        "plt.plot(fpr, tpr, color='blue', label=f'KNN (AUC = {roc_auc:.3f})')\n",
        "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
        "plt.title('ROC Curve for KNN Classifier')\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.legend(loc='lower right')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "O8ISjHmRsLr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example Output:**"
      ],
      "metadata": {
        "id": "pd62__jAsxAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ROC-AUC Score: 0.989"
      ],
      "metadata": {
        "id": "0wvgGJTus1qV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation:**\n",
        "* ROC-AUC close to 1.0 means excellent separation between positive and negative classes.\n",
        "* KNN did very well here — AUC of 0.989 is superb!\n",
        "* The ROC curve plots TPR vs FPR at various thresholds, giving a full picture of model performance."
      ],
      "metadata": {
        "id": "p5tvXhCTs4Sj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q44. Train a PCA model and visualize the variance captured by each principal component."
      ],
      "metadata": {
        "id": "kEPV8sTop_RI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans** - PCA is to visualize how much variance each principal component captures. This helps we decide how many components to keep while retaining most of the information.\n",
        "\n",
        "Let’s go through it step by step using the Wine dataset again."
      ],
      "metadata": {
        "id": "jDz3BMsi2TeS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
        "\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.bar(range(1, len(explained_variance_ratio)+1), explained_variance_ratio, alpha=0.7, align='center',\n",
        "        label='Individual explained variance')\n",
        "plt.step(range(1, len(cumulative_variance_ratio)+1), cumulative_variance_ratio, where='mid',\n",
        "         label='Cumulative explained variance', color='red')\n",
        "plt.xlabel('Principal Component Index')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.title('Variance Captured by Each Principal Component')\n",
        "plt.legend(loc='best')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5MFaovsmtLpT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation:**\n",
        "* Typically, we'll see a steep curve initially, capturing most variance in the first few components.\n",
        "* For example:\n",
        "  * PC1 might explain 36%\n",
        "  * PC2 might add 19%\n",
        "  * And so on…\n",
        "* We can decide to keep components covering, say, 95% of the variance."
      ],
      "metadata": {
        "id": "7L8YVMz1t2g7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q45. Train a KNN Classifier and perform feature selection before training"
      ],
      "metadata": {
        "id": "dSlimhmlp_hl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans** - KNN classifier can improve performance, reduce overfitting, and sometimes even speed up training.\n",
        "\n",
        "* Using the Wine dataset and SelectKBest feature selection."
      ],
      "metadata": {
        "id": "USlJFhyM2aPS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "selector = SelectKBest(score_func=f_classif, k=5)\n",
        "X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
        "X_test_selected = selector.transform(X_test_scaled)\n",
        "\n",
        "selected_features = selector.get_support(indices=True)\n",
        "print(\"Selected feature indices:\", selected_features)\n",
        "\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train_selected, y_train)\n",
        "\n",
        "y_pred = knn.predict(X_test_selected)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, target_names=wine.target_names))"
      ],
      "metadata": {
        "id": "ek2AfrItuxYq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example Output:**"
      ],
      "metadata": {
        "id": "8vdsDUrmvjj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Selected feature indices: [0 6 9 10 12]\n",
        "\n",
        "Accuracy: 0.972\n",
        "Classification Report:\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "     class_0       1.00      1.00      1.00        12\n",
        "     class_1       0.90      1.00      0.95         9\n",
        "     class_2       1.00      0.91      0.95        11"
      ],
      "metadata": {
        "id": "uIScs_kQvovC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation:**\n",
        "* Feature selection improved clarity and focus for KNN, which can struggle when irrelevant or noisy features are present.\n",
        "* Selected top 5 features using ANOVA F-values, which measure how strongly each feature is related to the target.\n",
        "* Accuracy and F1-scores remain high — sometimes even better with fewer, more relevant features."
      ],
      "metadata": {
        "id": "7ZFMoQABvrYS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 46. Train a PCA model and visualize the data reconstruction error after reducing dimensions"
      ],
      "metadata": {
        "id": "FDH9WYCrZlyn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans** - Analyzing the data reconstruction error after dimensionality reduction is a great way to understand how much information is lost when compressing data with PCA.\n",
        "\n",
        "Let's build this step-by-step using the Wine dataset — we'll:\n",
        "1. Reduce the number of dimensions with PCA\n",
        "2. Reconstruct the original data from the reduced representation\n",
        "3. Compute and visualize the reconstruction error\n",
        "\n",
        "**Required Libraries:**"
      ],
      "metadata": {
        "id": "nnetpfMd2iUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "components_list = range(1, X_scaled.shape[1] + 1)\n",
        "\n",
        "reconstruction_errors = []\n",
        "\n",
        "for n_components in components_list:\n",
        "\n",
        "    pca = PCA(n_components=n_components)\n",
        "    X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "    X_reconstructed = pca.inverse_transform(X_pca)\n",
        "\n",
        "    error = np.mean((X_scaled - X_reconstructed) ** 2)\n",
        "    reconstruction_errors.append(error)\n",
        "    plt.figure(figsize=(8, 5))\n",
        "plt.plot(components_list, reconstruction_errors, marker='o')\n",
        "plt.xlabel('Number of Principal Components')\n",
        "plt.ylabel('Reconstruction Error (MSE)')\n",
        "plt.title('PCA Data Reconstruction Error vs Number of Components')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "CKLHB7v3v_c6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation:**\n",
        "* we can decide how many components to keep by looking for the elbow point in the curve — a spot where adding more components doesn't significantly reduce error.\n",
        "* This gives a trade-off between dimensionality reduction and information loss."
      ],
      "metadata": {
        "id": "Hml01JXywsb6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Q 47. Train a KNN Classifier and visualize the decision boundary."
      ],
      "metadata": {
        "id": "5GlFlKRJZzHf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans** - Visualizing the decision boundary of a KNN classifier is an excellent way to understand how the classifier is making decisions across different regions of the feature space. To make this visualization easier, let's reduce the number of features to two using PCA for visualization purposes, and then plot the decision boundary.\n",
        "\n",
        "We'll use the Wine dataset, but for simplicity, we'll select only the first two principal components to reduce the data to two dimensions.\n",
        "\n",
        "**Required Libraries:**"
      ],
      "metadata": {
        "id": "afYW7mUa2vQy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "x_min, x_max = X_pca[:, 0].min() - 1, X_pca[:, 0].max() + 1\n",
        "y_min, y_max = X_pca[:, 1].min() - 1, X_pca[:, 1].max() + 1\n",
        "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1),\n",
        "                     np.arange(y_min, y_max, 0.1))\n",
        "\n",
        "Z = knn.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "Z = Z.reshape(xx.shape)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.RdYlBu)\n",
        "\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolor='k', marker='o', s=100, cmap=plt.cm.RdYlBu, label=\"Train\")\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, edgecolor='k', marker='s', s=100, cmap=plt.cm.RdYlBu, label=\"Test\")\n",
        "\n",
        "plt.title(\"KNN Classifier Decision Boundary (k=5) with PCA Transformed Features\")\n",
        "plt.xlabel(\"Principal Component 1\")\n",
        "plt.ylabel(\"Principal Component 2\")\n",
        "plt.legend(loc=\"best\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "cI5xCEDsxB46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation:**\n",
        "* The KNN model makes local decisions based on the majority vote of the nearest neighbors, which is clearly visualized in the decision boundaries.\n",
        "* The sharp boundaries might indicate that KNN is highly sensitive to feature scaling and choice of k."
      ],
      "metadata": {
        "id": "HJSF8HX_xwch"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q48. Train a PCA model and analyze the effect of different numbers of components on data variance."
      ],
      "metadata": {
        "id": "6enKtHG6aPLn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ans** - The effect of different numbers of components on data variance in PCA can help us to understand how much information is retained as we reduce dimensionality.\n",
        "\n",
        "We'll use the Wine dataset again and explore the following steps:\n",
        "1. Apply PCA for varying numbers of components.\n",
        "2. Compute and visualize the explained variance ratio for each component.\n",
        "3. Analyze how the total variance captured by the components increases as we add more components.\n",
        "\n",
        "**Required Libraries:**"
      ],
      "metadata": {
        "id": "BWy2sj2Q2-NS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "pca = PCA()\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "\n",
        "cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.bar(range(1, len(explained_variance_ratio) + 1), explained_variance_ratio, alpha=0.7, align='center')\n",
        "plt.xlabel('Principal Component')\n",
        "plt.ylabel('Explained Variance Ratio')\n",
        "plt.title('Explained Variance by Each Principal Component')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.plot(range(1, len(cumulative_variance_ratio) + 1), cumulative_variance_ratio, marker='o', color='r')\n",
        "plt.xlabel('Number of Components')\n",
        "plt.ylabel('Cumulative Explained Variance Ratio')\n",
        "plt.title('Cumulative Explained Variance')\n",
        "\n",
        "plt.tight_lawet()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Q2f5245GyJ0S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Interpretation:**\n",
        "* The first 1 or 2 components typically capture most of the variance.\n",
        "* Cumulative variance curve will help we decide how many components to keep to retain a certain level of information.\n",
        "* This is useful for dimensionality reduction, where we aim to reduce dimensions without losing too much information."
      ],
      "metadata": {
        "id": "b0PX7vocy4FR"
      }
    }
  ]
}